"""
ìŠ¤í…”ìŠ¤ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬
ê·¹ë„ë¡œ ê°•í™”ëœ IP ì°¨ë‹¨ ìš°íšŒ ê¸°ë²•ë“¤ì„ í¬í•¨
"""

import requests
import pandas as pd
import json
import time
import random
import hashlib
import base64
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib3.exceptions import InsecureRequestWarning
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# SSL ê²½ê³  ë¹„í™œì„±í™”
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# --- ì„¤ì • ë¶€ë¶„ ---
PRODUCT_ID = "5753732771"  # ë¶„ì„í•˜ê³  ì‹¶ì€ ìƒí’ˆì˜ ID
OUTPUT_FILE_NAME = "reviews.csv"
NUM_TOPICS = 5

# ê·¹ë„ë¡œ ë‹¤ì–‘í•œ User-Agent ëª©ë¡ (ìµœì‹  ë¸Œë¼ìš°ì € í†µê³„ ê¸°ë°˜)
STEALTH_USER_AGENTS = [
    # í•œêµ­ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¸Œë¼ìš°ì €ë“¤
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0",
    
    # ëª¨ë°”ì¼ ë¸Œë¼ìš°ì €ë“¤ (í•œêµ­ ì„ í˜¸ë„ ë†’ìŒ)
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
    
    # íƒœë¸”ë¦¿
    "Mozilla/5.0 (iPad; CPU OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
]

# ë¬´ë£Œ í”„ë¡ì‹œ ì„œë²„ë“¤ (ì‹¤ì œ ì‚¬ìš©ì‹œ ê²€ì¦ í•„ìš”)
FREE_PROXIES = [
    # ì—¬ê¸°ì— ì‹¤ì œ ì‘ë™í•˜ëŠ” í”„ë¡ì‹œ ì£¼ì†Œë“¤ì„ ì¶”ê°€
    # {'http': 'http://proxy:port', 'https': 'https://proxy:port'},
]

class StealthNaverCrawler:
    def __init__(self, product_id):
        self.product_id = product_id
        self.session = None
        self.request_count = 0
        self.last_request_time = 0
        self.current_proxy = None
        self.failed_proxies = set()
        self.session_start_time = time.time()
        
    def _create_stealth_session(self):
        """ê·¹ë„ë¡œ ì€ë°€í•œ ì„¸ì…˜ ìƒì„±"""
        session = requests.Session()
        
        # ë§¤ìš° ê´€ëŒ€í•œ ì¬ì‹œë„ ì „ëµ
        retry_strategy = Retry(
            total=10,  # ìµœëŒ€ 10ë²ˆ ì¬ì‹œë„
            backoff_factor=3,  # 3ì´ˆì”© ì¦ê°€
            status_forcelist=[403, 429, 500, 502, 503, 504, 520, 521, 522, 523, 524],
            allowed_methods=["HEAD", "GET", "POST", "PUT", "DELETE", "OPTIONS", "TRACE"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=20,
            pool_maxsize=50,
            pool_block=True
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # ê³ ê¸‰ ì„¸ì…˜ ì„¤ì •
        session.verify = False
        session.trust_env = False
        session.max_redirects = 10
        
        return session
    
    def _rotate_proxy(self):
        """í”„ë¡ì‹œ ë¡œí…Œì´ì…˜"""
        if not FREE_PROXIES:
            return None
            
        available_proxies = [p for p in FREE_PROXIES if str(p) not in self.failed_proxies]
        
        if available_proxies:
            return random.choice(available_proxies)
        else:
            # ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í•œ ê²½ìš° ì´ˆê¸°í™”
            self.failed_proxies.clear()
            return random.choice(FREE_PROXIES)
    
    def _generate_stealth_headers(self, referer=None, is_mobile=False):
        """ê·¹ë„ë¡œ ì •êµí•œ ìŠ¤í…”ìŠ¤ í—¤ë” ìƒì„±"""
        user_agent = random.choice(STEALTH_USER_AGENTS)
        is_mobile_ua = "Mobile" in user_agent or "iPhone" in user_agent or "Android" in user_agent
        
        # ê¸°ë³¸ í—¤ë”
        headers = {
            "User-Agent": user_agent,
            "Accept": "application/json, text/plain, */*" if not is_mobile_ua else "application/json, text/javascript, */*; q=0.01",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
        }
        
        # Referer ì„¤ì •
        if referer:
            headers["Referer"] = referer
        else:
            # ëœë¤ Referer ìƒì„±
            referers = [
                f"https://smartstore.naver.com/i/v1/products/{self.product_id}",
                "https://search.shopping.naver.com/",
                "https://www.naver.com/",
                "https://m.shopping.naver.com/",
            ]
            headers["Referer"] = random.choice(referers)
        
        # ë¸Œë¼ìš°ì €ë³„ íŠ¹í™” í—¤ë”
        if "Chrome" in user_agent and "Edge" not in user_agent:
            headers.update({
                "sec-ch-ua": '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',
                "sec-ch-ua-mobile": "?1" if is_mobile_ua else "?0",
                "sec-ch-ua-platform": '\"Android\"' if "Android" in user_agent else ('\"iOS\"' if "iPhone" in user_agent else '\"Windows\"'),
                "Sec-Fetch-Dest": "empty",
                "Sec-Fetch-Mode": "cors",
                "Sec-Fetch-Site": "same-origin",
            })
        elif "Firefox" in user_agent:
            headers.update({
                "DNT": "1",
                "Sec-Fetch-Dest": "empty",
                "Sec-Fetch-Mode": "cors",
                "Sec-Fetch-Site": "same-origin",
            })
        elif "Safari" in user_agent and "Chrome" not in user_agent:
            headers.update({
                "Accept": "application/json, text/javascript, */*; q=0.01",
            })
        
        # ëª¨ë°”ì¼ íŠ¹í™” í—¤ë”
        if is_mobile_ua:
            headers.update({
                "X-Requested-With": "XMLHttpRequest",
                "Origin": "https://m.smartstore.naver.com" if random.random() > 0.5 else "https://smartstore.naver.com",
            })
        
        # ëœë¤ ì¶”ê°€ í—¤ë”ë“¤
        random_headers = {}
        
        if random.random() > 0.3:
            random_headers["DNT"] = "1"
        
        if random.random() > 0.4:
            random_headers["X-Forwarded-For"] = f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"
        
        if random.random() > 0.5:
            random_headers["X-Real-IP"] = f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"
        
        if random.random() > 0.6:
            random_headers["CF-Connecting-IP"] = f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"
        
        headers.update(random_headers)
        
        return headers
    
    def _extreme_delay(self):
        """ê·¹ë„ë¡œ ì •êµí•œ ì§€ì—° ì‹œìŠ¤í…œ"""
        current_time = time.time()
        
        # ì„¸ì…˜ ì§€ì† ì‹œê°„ì— ë”°ë¥¸ ì§€ì—° ì¡°ì •
        session_duration = current_time - self.session_start_time
        
        if session_duration > 3600:  # 1ì‹œê°„ ì´ìƒ
            self.session_start_time = current_time
            self.request_count = 0
            print("ğŸ”„ ì„¸ì…˜ ë¦¬ì…‹ - ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘")
        
        # ìš”ì²­ ë¹ˆë„ì— ë”°ë¥¸ ì§€ì—°
        base_delay = 3
        if self.request_count > 50:
            base_delay = 10
        elif self.request_count > 20:
            base_delay = 7
        elif self.request_count > 10:
            base_delay = 5
        
        # ì‹œê°„ëŒ€ë³„ ì§€ì—° (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
        current_hour = datetime.now().hour
        if 9 <= current_hour <= 18:  # ì—…ë¬´ì‹œê°„
            time_multiplier = 2.0
        elif 19 <= current_hour <= 23:  # ì €ë…ì‹œê°„
            time_multiplier = 1.5
        else:  # ìƒˆë²½/ë°¤
            time_multiplier = 1.0
        
        # ëœë¤ ì§€ì—° ê³„ì‚°
        delay = random.uniform(base_delay * time_multiplier, (base_delay + 5) * time_multiplier)
        
        # ì´ì „ ìš”ì²­ìœ¼ë¡œë¶€í„°ì˜ ìµœì†Œ ê°„ê²© ë³´ì¥
        if self.last_request_time > 0:
            elapsed = current_time - self.last_request_time
            if elapsed < delay:
                actual_delay = delay - elapsed
                print(f"â³ {actual_delay:.2f}ì´ˆ ëŒ€ê¸° (ìš”ì²­#{self.request_count}, ì‹œê°„ëŒ€ ê³„ìˆ˜: {time_multiplier:.1f})")
                time.sleep(actual_delay)
        
        self.last_request_time = time.time()
        self.request_count += 1
    
    def _simulate_human_behavior(self):
        """ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜"""
        behaviors = [
            self._visit_main_page,
            self._search_random_product,
            self._visit_category_page,
        ]
        
        # 20% í™•ë¥ ë¡œ ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
        if random.random() < 0.2:
            behavior = random.choice(behaviors)
            try:
                behavior()
            except:
                pass
    
    def _visit_main_page(self):
        """ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸"""
        try:
            headers = self._generate_stealth_headers()
            self.session.get("https://smartstore.naver.com/", headers=headers, timeout=10)
            time.sleep(random.uniform(2, 5))
        except:
            pass
    
    def _search_random_product(self):
        """ëœë¤ ìƒí’ˆ ê²€ìƒ‰"""
        try:
            keywords = ["íŒ¨ì…˜", "ë·°í‹°", "ìƒí™œ", "ë””ì§€í„¸", "ìŠ¤í¬ì¸ ", "ë°˜ë ¤ë™ë¬¼"]
            keyword = random.choice(keywords)
            headers = self._generate_stealth_headers()
            self.session.get(f"https://search.shopping.naver.com/search/all?query={keyword}", headers=headers, timeout=10)
            time.sleep(random.uniform(1, 3))
        except:
            pass
    
    def _visit_category_page(self):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ë°©ë¬¸"""
        try:
            headers = self._generate_stealth_headers()
            self.session.get("https://shopping.naver.com/category", headers=headers, timeout=10)
            time.sleep(random.uniform(1, 4))
        except:
            pass
    
    def get_product_info_stealth(self):
        """ìŠ¤í…”ìŠ¤ ìƒí’ˆ ì •ë³´ íšë“"""
        print("ğŸ•µï¸  ìŠ¤í…”ìŠ¤ ëª¨ë“œë¡œ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        # ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±
        self.session = self._create_stealth_session()
        
        # ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
        self._simulate_human_behavior()
        
        info_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}/summary"
        
        for attempt in range(15):  # ìµœëŒ€ 15ë²ˆ ì‹œë„
            try:
                self._extreme_delay()
                
                # í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
                self.current_proxy = self._rotate_proxy()
                
                headers = self._generate_stealth_headers(
                    referer=f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
                )
                
                print(f"ğŸ¯ ì‹œë„ {attempt + 1}: {headers.get('User-Agent', '')[:50]}...")
                
                response = self.session.get(
                    info_url,
                    headers=headers,
                    proxies=self.current_proxy,
                    timeout=30,
                    allow_redirects=True,
                    stream=False
                )
                
                if response.status_code == 200:
                    try:
                        product_info = response.json()
                        merchant_no = product_info['product']['channel']['channelNo']
                        origin_product_no = product_info['product']['productNo']
                        print(f"âœ… ìƒí’ˆ ì •ë³´ íšë“ ì„±ê³µ!")
                        print(f"   ğŸ“Š Merchant No: {merchant_no}")
                        print(f"   ğŸ“Š Origin Product No: {origin_product_no}")
                        return merchant_no, origin_product_no
                    except (json.JSONDecodeError, KeyError) as e:
                        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
                elif response.status_code == 403:
                    if self.current_proxy:
                        self.failed_proxies.add(str(self.current_proxy))
                    delay = min(300 * (attempt + 1), 1800)  # ìµœëŒ€ 30ë¶„
                    print(f"ğŸš« 403 Forbidden - {delay//60}ë¶„ ëŒ€ê¸°...")
                    time.sleep(delay)
                    
                elif response.status_code == 429:
                    delay = min(600 * (2 ** (attempt // 3)), 3600)  # ìµœëŒ€ 1ì‹œê°„
                    print(f"âš ï¸  429 Rate Limit - {delay//60}ë¶„ ëŒ€ê¸°...")
                    time.sleep(delay)
                    
                elif response.status_code == 404:
                    print(f"âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (404)")
                    return None, None
                    
                else:
                    print(f"âŒ HTTP {response.status_code}")
                    time.sleep(random.uniform(30, 120))
                
                # ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜ (ì¤‘ê°„ì—)
                if attempt % 3 == 0:
                    self._simulate_human_behavior()
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)[:100]}...")
                if self.current_proxy:
                    self.failed_proxies.add(str(self.current_proxy))
                time.sleep(random.uniform(60, 180))
        
        print("âŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ - ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None, None
    
    def crawl_reviews_stealth(self):
        """ìŠ¤í…”ìŠ¤ ë¦¬ë·° í¬ë¡¤ë§"""
        merchant_no, origin_product_no = self.get_product_info_stealth()
        
        if not merchant_no or not origin_product_no:
            return None
        
        all_reviews = []
        page = 1
        consecutive_failures = 0
        max_consecutive_failures = 5
        
        print("ğŸ•µï¸  ìŠ¤í…”ìŠ¤ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
        
        while True:
            url = (
                f"https://smartstore.naver.com/main/products/{origin_product_no}/reviews/writable-reviews"
                f"?page={page}&sort=REVIEW_RANKING"
                f"&merchantNo={merchant_no}"
            )
            
            try:
                self._extreme_delay()
                
                # í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
                self.current_proxy = self._rotate_proxy()
                
                headers = self._generate_stealth_headers(
                    referer=f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
                )
                
                response = self.session.get(
                    url,
                    headers=headers,
                    proxies=self.current_proxy,
                    timeout=30
                )
                
                if response.status_code == 200:
                    consecutive_failures = 0
                    
                    try:
                        data = response.json()
                        reviews = data.get('contents', [])
                        
                        if not reviews:
                            print("âœ… ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                            break
                        
                        for review in reviews:
                            option_text = " / ".join([
                                opt['optionContent'] 
                                for opt in review.get('productOptionContents', []) 
                                if 'optionContent' in opt
                            ])
                            
                            all_reviews.append({
                                'id': review.get('id'),
                                'rating': review.get('reviewScore'),
                                'writer': review.get('writerMemberId'),
                                'date': review.get('createDate'),
                                'content': review.get('reviewContent', ''),
                                'option': option_text,
                            })
                        
                        print(f"ğŸ“ í˜ì´ì§€ {page}: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ (ì´ {len(all_reviews)}ê°œ)")
                        page += 1
                        
                        # ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜ (ê°€ë”)
                        if page % 5 == 0:
                            self._simulate_human_behavior()
                        
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        consecutive_failures += 1
                
                elif response.status_code == 403:
                    print("ğŸš« 403 Forbidden - ê¸´ ëŒ€ê¸°...")
                    if self.current_proxy:
                        self.failed_proxies.add(str(self.current_proxy))
                    time.sleep(random.uniform(600, 1200))  # 10-20ë¶„
                    consecutive_failures += 1
                    
                elif response.status_code == 429:
                    print("âš ï¸  429 Rate Limit - ë§¤ìš° ê¸´ ëŒ€ê¸°...")
                    time.sleep(random.uniform(1200, 1800))  # 20-30ë¶„
                    consecutive_failures += 1
                    
                else:
                    print(f"âŒ HTTP {response.status_code}")
                    consecutive_failures += 1
                    time.sleep(random.uniform(120, 300))
                
                if consecutive_failures >= max_consecutive_failures:
                    print(f"âŒ {max_consecutive_failures}íšŒ ì—°ì† ì‹¤íŒ¨ë¡œ ì¤‘ë‹¨")
                    break
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)[:100]}...")
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    break
                time.sleep(random.uniform(180, 360))
        
        if all_reviews:
            print(f"ğŸ‰ ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            return pd.DataFrame(all_reviews)
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None


def analyze_sentiment(text, positive_keywords, negative_keywords):
    """ê°ì„± ë¶„ì„"""
    score = 0
    for keyword in positive_keywords:
        if keyword in text:
            score += 1
    for keyword in negative_keywords:
        if keyword in text:
            score -= 1
    
    if score > 0:
        return 'ê¸ì •'
    elif score < 0:
        return 'ë¶€ì •'
    else:
        return 'ì¤‘ë¦½'


def topic_modeling(df, num_topics):
    """í† í”½ ëª¨ë¸ë§"""
    okt = Okt()
    
    def tokenize(text):
        if isinstance(text, str):
            return [token for token in okt.nouns(text) if len(token) > 1]
        return []

    df['tokens'] = df['content'].apply(tokenize)
    
    texts = [" ".join(tokens) for tokens in df['tokens']]

    if not any(texts):
        print("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í† í”½ ëª¨ë¸ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        df['topic'] = 'ë¶„ì„ ë¶ˆê°€'
        return df

    vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2)
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(tfidf_matrix)
    
    topic_results = lda.transform(tfidf_matrix)
    df['topic'] = topic_results.argmax(axis=1)

    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(lda.components_):
        top_keywords = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]
        print(f"í† í”½ #{topic_idx}: {', '.join(top_keywords)}")
        
    return df


if __name__ == '__main__':
    print("ğŸ•µï¸  === ìŠ¤í…”ìŠ¤ ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ ì‹œì‘ ===")
    print(f"ğŸ¯ íƒ€ê²Ÿ ìƒí’ˆ: {PRODUCT_ID}")
    print("âš ï¸  ì´ í¬ë¡¤ëŸ¬ëŠ” ê·¹ë„ë¡œ ì‹ ì¤‘í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = StealthNaverCrawler(PRODUCT_ID)
    
    # ë¦¬ë·° í¬ë¡¤ë§
    review_df = crawler.crawl_reviews_stealth()
    
    if review_df is not None and not review_df.empty:
        print("\nğŸ“Š === ë°ì´í„° ë¶„ì„ ì‹œì‘ ===")
        
        # ê°ì„± ë¶„ì„
        positive_keywords = ['ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ë¹ ë¥¸', 'í¸í•˜ê³ ', 'ì˜ˆë»ìš”', 'ê°€ë³ê³ ', 'íŠ¼íŠ¼', 'ì˜', 'ë§˜ì—']
        negative_keywords = ['ë¶ˆí¸', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬', 'ë¶ˆë§Œ', 'ëŠë¦°', 'ë¬´ê±°', 'ì•½í•œ', 'ë¬¸ì œ', 'ê³ ì¥']
        
        review_df['sentiment'] = review_df['content'].apply(
            lambda x: analyze_sentiment(x, positive_keywords, negative_keywords)
        )
        
        print("\n--- ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼ ---")
        print(review_df['sentiment'].value_counts())
        
        # í† í”½ ëª¨ë¸ë§
        print("\n--- ğŸ·ï¸  í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---")
        review_df = topic_modeling(review_df, NUM_TOPICS)
        
        # ê²°ê³¼ ì €ì¥
        review_df.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # í†µê³„ ì •ë³´
        print(f"\n--- ğŸ“Š ìˆ˜ì§‘ í†µê³„ ---")
        print(f"ğŸ“ ì´ ë¦¬ë·° ìˆ˜: {len(review_df)}")
        print(f"â­ í‰ê·  í‰ì : {review_df['rating'].mean():.2f}")
        print(f"ğŸ˜Š ê¸ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ê¸ì •'])}")
        print(f"ğŸ˜ ë¶€ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ë¶€ì •'])}")
        print(f"ğŸ˜ ì¤‘ë¦½ ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ì¤‘ë¦½'])}")
        
    else:
        print("\nâŒ === í¬ë¡¤ë§ ì‹¤íŒ¨ ===")
        print("ğŸ”§ í•´ê²° ë°©ì•ˆ:")
        print("1. VPN ì‚¬ìš© ê³ ë ¤")
        print("2. ë‹¤ë¥¸ ì‹œê°„ëŒ€ì— ì¬ì‹œë„ (ìƒˆë²½ 2-6ì‹œ ê¶Œì¥)")
        print("3. í”„ë¡ì‹œ ì„œë²„ ì¶”ê°€")
        print("4. ìƒí’ˆ ID í™•ì¸")