import requests
import pandas as pd
import json
import time
import random
import re
import subprocess
import os
import schedule
import threading
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Tuple

try:
    from stealth_crawler import StealthNaverCrawler
    from selenium_crawler import SeleniumNaverCrawler
    from mobile_crawler import MobileNaverCrawler
    from advanced_crawler import AdvancedNaverCrawler
    CRAWLERS_AVAILABLE = True
except ImportError:
    CRAWLERS_AVAILABLE = False
    print("âš ï¸  í¬ë¡¤ëŸ¬ ëª¨ë“ˆë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë™ì¼í•œ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

def _is_valid_time_format(time_str: str) -> bool:
    """ 'HH:MM' í˜•ì‹ì¸ì§€ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜ """
    return bool(re.fullmatch(r"([01]?[0-9]|2[0-3]):[0-5][0-9]", time_str.strip()))

class SmartCrawlerScheduler:
    def __init__(self, config_file="crawler_config.json"):
        self.config_file = config_file
        self.config = self._load_config()
        self.setup_logging()

    def _load_config(self) -> Dict:
        default_config = {
            "schedule": {"auto_run_times": ["02:00", "03:30", "05:00"], "retry_interval_hours": 6},
            "vpn": {"enabled": False, "provider": "expressvpn", "countries": ["japan", "singapore"], "connect_command": "expressvpn connect {country}", "disconnect_command": "expressvpn disconnect", "status_command": "expressvpn status"},
            "crawlers": {"priority_order": ["stealth", "selenium", "mobile", "advanced"], "max_retries_per_crawler": 2, "delay_between_crawlers": 300},
            "output": {"base_directory": "crawl_results", "filename_pattern": "{product_id}_{timestamp}_{crawler}.csv", "keep_logs_days": 30},
            "products": []
        }
        if not os.path.exists(self.config_file):
            self._save_config(default_config)
            return default_config
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëˆ„ë½ëœ í‚¤ ë³´ì™„
            for key, value in default_config.items():
                if key not in config: config[key] = value
                elif isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        if sub_key not in config.get(key, {}): config[key][sub_key] = sub_value
            return config
        except Exception as e:
            print(f"âš ï¸  ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return default_config

    def _save_config(self, config: Dict):
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def setup_logging(self):
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / f"crawler_scheduler_{datetime.now().strftime('%Y%m%d')}.log"
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler()])
        self.logger = logging.getLogger(__name__)
    
    def extract_product_id(self, url: str) -> Optional[str]:
        patterns = [
            r'smartstore\.naver\.com/[^/]+/products/(\d+)', r'shopping\.naver\.com/.*?nvMid=(\d+)',
            r'm\.smartstore\.naver\.com/.*?/products/(\d+)', r'm\.shopping\.naver\.com/.*?nvMid=(\d+)',
            r'[?&]productId=(\d+)', r'[?&]id=(\d+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                self.logger.info(f"âœ… URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ ì„±ê³µ: {match.group(1)}")
                return match.group(1)
        self.logger.error(f"âŒ URLì—ì„œ ìƒí’ˆ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
        return None

    def add_product(self, url: str, name: str = "", priority: int = 1) -> bool:
        product_id = self.extract_product_id(url)
        if not product_id: return False
        
        products = self.config.get('products', [])
        if any(p.get('id') == product_id for p in products):
            self.logger.info(f"âš ï¸  ìƒí’ˆ {product_id}ëŠ” ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True # ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        product = {"id": product_id, "url": url, "name": name or f"ìƒí’ˆ_{product_id}", "priority": priority, "added_date": datetime.now().isoformat(), "last_crawl": None, "success_count": 0, "fail_count": 0, "enabled": True}
        products.append(product)
        self.config['products'] = products
        self._save_config(self.config)
        self.logger.info(f"âœ… ìƒí’ˆ ì¶”ê°€ ì™„ë£Œ: {name} ({product_id})")
        return True

    def remove_product(self, product_id: str) -> bool:
        products = self.config.get('products', [])
        original_count = len(products)
        self.config['products'] = [p for p in products if p.get('id') != product_id]
        if len(self.config['products']) < original_count:
            self._save_config(self.config)
            self.logger.info(f"âœ… ìƒí’ˆ {product_id} ì œê±° ì™„ë£Œ")
            return True
        return False
    
    def list_products(self) -> List[Dict]:
        return self.config.get('products', [])

    def connect_vpn(self) -> bool:
        vpn_config = self.config.get('vpn', {})
        if not vpn_config.get('enabled'): return True
        try:
            country = random.choice(vpn_config.get('countries', []))
            command = vpn_config.get('connect_command', "").format(country=country)
            self.logger.info(f"ğŸ”— VPN ì—°ê²° ì‹œë„: {country}")
            result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                self.logger.info(f"âœ… VPN ì—°ê²° ì„±ê³µ: {country}"); time.sleep(10); return True
            self.logger.error(f"âŒ VPN ì—°ê²° ì‹¤íŒ¨: {result.stderr or result.stdout}"); return False
        except Exception as e:
            self.logger.error(f"âŒ VPN ì—°ê²° ì˜¤ë¥˜: {e}"); return False

    def disconnect_vpn(self) -> bool:
        vpn_config = self.config.get('vpn', {})
        if not vpn_config.get('enabled'): return True
        try:
            command = vpn_config.get('disconnect_command', "")
            self.logger.info("ğŸ”Œ VPN ì—°ê²° í•´ì œ ì¤‘...")
            result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=30)
            if result.returncode == 0: self.logger.info("âœ… VPN ì—°ê²° í•´ì œ ì™„ë£Œ"); return True
            self.logger.error(f"âŒ VPN í•´ì œ ì‹¤íŒ¨: {result.stderr or result.stdout}"); return False
        except Exception as e:
            self.logger.error(f"âŒ VPN í•´ì œ ì˜¤ë¥˜: {e}"); return False

    def get_vpn_status(self) -> str:
        vpn_config = self.config.get('vpn', {})
        if not vpn_config.get('enabled'): return "ë¹„í™œì„±í™”ë¨"
        try:
            command = vpn_config.get('status_command', "")
            result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10)
            return result.stdout.strip()
        except: return "ìƒíƒœ í™•ì¸ ë¶ˆê°€"

    def crawl_product(self, product: Dict) -> Optional[str]:
        product_id = product.get("id")
        self.logger.info(f"ğŸ¯ í¬ë¡¤ë§ ì‹œì‘: {product.get('name', '')} ({product_id})")
        
        vpn_config = self.config.get("vpn", {})
        if vpn_config.get("enabled"):
            if not self.connect_vpn():
                self.logger.error("âŒ VPN ì—°ê²° ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨")
                return None
        
        success_file = None
        crawler_config = self.config.get('crawlers', {})
        crawler_order = crawler_config.get('priority_order', [])
        
        try:
            for crawler_name in crawler_order:
                for retry in range(crawler_config.get('max_retries_per_crawler', 1)):
                    self.logger.info(f"ğŸ¤– {crawler_name} í¬ë¡¤ëŸ¬ ì‹œë„ ({retry + 1})")
                    result_path, status_code = self._run_crawler(crawler_name, product_id)
                    
                    if result_path:
                        success_file = result_path
                        self.logger.info(f"âœ… {crawler_name} í¬ë¡¤ëŸ¬ë¡œ ì„±ê³µ!")
                        product['success_count'] = product.get('success_count', 0) + 1
                        break # ì„±ê³µ ì‹œ ë‹¤ìŒ í¬ë¡¤ëŸ¬ë¡œ ë„˜ì–´ê°€ì§€ ì•ŠìŒ
                    
                    self.logger.warning(f"âš ï¸ {crawler_name} í¬ë¡¤ëŸ¬ ì‹¤íŒ¨ (ìƒíƒœ: {status_code})")
                    if status_code in [403, 429] and vpn_config.get("enabled"):
                        self.logger.warning("ğŸš« IP ì°¨ë‹¨ ê°€ëŠ¥ì„±. VPN ì¬ì—°ê²° ì‹œë„.")
                        self.disconnect_vpn(); time.sleep(5); self.connect_vpn()
                if success_file: break
            
            if not success_file:
                product['fail_count'] = product.get('fail_count', 0) + 1
                self.logger.error(f"âŒ ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {product.get('name')}")
            product['last_crawl'] = datetime.now().isoformat()
            self._save_config(self.config)
        finally:
            if vpn_config.get("enabled"):
                self.disconnect_vpn()
        return success_file
    
    def _run_crawler(self, crawler_name: str, product_id: str) -> Tuple[Optional[str], Optional[int]]:
        if not CRAWLERS_AVAILABLE: return None, None
        
        output_config = self.config.get('output', {})
        output_dir = Path(output_config.get('base_directory', 'crawl_results'))
        output_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_pattern = output_config.get('filename_pattern', '{product_id}_{timestamp}_{crawler}.csv')
        filename = filename_pattern.format(product_id=product_id, timestamp=timestamp, crawler=crawler_name)
        output_file = output_dir / filename
        status_code = None

        try:
            crawler_map = {
                "advanced": AdvancedNaverCrawler, "stealth": StealthNaverCrawler,
                "mobile": MobileNaverCrawler, "selenium": SeleniumNaverCrawler,
            }
            if crawler_name not in crawler_map:
                self.logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” í¬ë¡¤ëŸ¬: {crawler_name}"); return None, None

            crawler_instance = crawler_map[crawler_name](product_id)
            
            # ê° í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ì˜ ì •ë³´ íšë“ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒíƒœ ì½”ë“œ í™•ì¸
            info_method_map = {
                "advanced": "get_product_info", "stealth": "get_product_info_stealth",
                "mobile": "get_product_info_mobile", "selenium": "get_product_info"
            }
            if hasattr(crawler_instance, info_method_map[crawler_name]):
                 _, _, status_code = getattr(crawler_instance, info_method_map[crawler_name])()
            
            result_df = None
            if status_code == 200:
                result_df = crawler_instance.crawl_reviews()

            if result_df is not None and not result_df.empty:
                result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
                return str(output_file), 200
            else:
                return None, status_code
        except Exception as e:
            self.logger.error(f"âŒ {crawler_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None, status_code

    def start_scheduler(self):
        self.logger.info("ğŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - Ctrl+Cë¡œ ì¤‘ë‹¨")
        schedule_times = self.config.get('schedule', {}).get('auto_run_times', [])
        for run_time in schedule_times:
            if _is_valid_time_format(run_time):
                schedule.every().day.at(run_time).do(self.crawl_all_products)
                self.logger.info(f"â° ìŠ¤ì¼€ì¤„ ë“±ë¡: ë§¤ì¼ {run_time}")
        try:
            while True:
                schedule.run_pending(); time.sleep(60)
        except KeyboardInterrupt:
            self.logger.info("â›” ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ë‹¨ë¨")

    def crawl_all_products(self):
        self.logger.info(f"ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        active_products = [p for p in self.list_products() if p.get("enabled", True)]
        for product in active_products:
            self.crawl_product(product)

    def manual_crawl(self, product_id_or_url: str) -> Optional[str]:
        if product_id_or_url.startswith("http"):
            product_id = self.extract_product_id(product_id_or_url)
            if not product_id: return None
        else:
            product_id = product_id_or_url
        
        temp_product = {"id": product_id, "name": f"ìˆ˜ë™_{product_id}", "url": product_id_or_url}
        return self.crawl_product(temp_product)

if __name__ == "__main__":
    scheduler = SmartCrawlerScheduler()
    while True:
        print("\nğŸ¤– === ìŠ¤ë§ˆíŠ¸ ë„¤ì´ë²„ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ===")
        print("1. ìƒí’ˆ ì¶”ê°€")
        print("2. ìƒí’ˆ ëª©ë¡ ë³´ê¸°")
        print("3. ìƒí’ˆ ì œê±°")
        print("4. VPN/ìŠ¤ì¼€ì¤„ ì„¤ì •")
        print("5. ìˆ˜ë™ í¬ë¡¤ë§ (URL/ID ì…ë ¥)")
        print("6. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ìë™ ì‹¤í–‰)")
        print("7. ì „ì²´ ìƒí’ˆ ì¦‰ì‹œ í¬ë¡¤ë§")
        print("0. ì¢…ë£Œ")
        choice = input("ì„ íƒí•˜ì„¸ìš”: ").strip()

        if choice == '1':
            url = input("ìƒí’ˆ URL: ").strip()
            name = input("ìƒí’ˆ ì´ë¦„ (ì„ íƒì‚¬í•­): ").strip()
            scheduler.add_product(url, name)
        elif choice == '2':
            for p in scheduler.list_products(): print(p)
        elif choice == '3':
            pid = input("ì œê±°í•  ìƒí’ˆ ID: ").strip()
            scheduler.remove_product(pid)
        elif choice == '4':
            print("--- VPN ì„¤ì • ---")
            provider = input("VPN ì œê³µì—…ì²´ (expressvpn/nordvpn/surfshark): ").strip()
            countries_str = input("êµ­ê°€ ëª©ë¡ (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
            if provider and countries_str:
                scheduler.config['vpn']['provider'] = provider
                scheduler.config['vpn']['countries'] = [c.strip() for c in countries_str.split(',')]
                scheduler.config['vpn']['enabled'] = True
            print("--- ìŠ¤ì¼€ì¤„ ì„¤ì • ---")
            times_str = input("ìë™ ì‹¤í–‰ ì‹œê°„ (HH:MM, ì‰¼í‘œ êµ¬ë¶„): ").strip()
            times = [t.strip() for t in times_str.split(',')]
            if all(_is_valid_time_format(t) for t in times):
                scheduler.config['schedule']['auto_run_times'] = times
                print("âœ… ìŠ¤ì¼€ì¤„ ì‹œê°„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì˜ëª»ëœ ì‹œê°„ í˜•ì‹ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            scheduler._save_config(scheduler.config)
        elif choice == '5':
            url_or_id = input("í¬ë¡¤ë§í•  URL ë˜ëŠ” ìƒí’ˆ ID: ").strip()
            scheduler.manual_crawl(url_or_id)
        elif choice == '6':
            scheduler.start_scheduler()
        elif choice == '7':
            scheduler.crawl_all_products()
        elif choice == '0':
            break