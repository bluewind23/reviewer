"""
ìŠ¤ë§ˆíŠ¸ ë„¤ì´ë²„ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬
- URLì—ì„œ ìƒí’ˆì½”ë“œ ìë™ ì¶”ì¶œ
- ì •í•´ì§„ ì‹œê°„ì— ìë™ ì‹¤í–‰
- VPN ìë™ ì—°ê²°/í•´ì œ
- ë‹¤ì¤‘ í¬ë¡¤ëŸ¬ í†µí•© ê´€ë¦¬
"""

import requests
import pandas as pd
import json
import time
import random
import re
import subprocess
import os
import schedule
import threading
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs
from pathlib import Path
import logging
from typing import Optional, Dict, List, Tuple

# ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ import
try:
    from stealth_crawler import StealthNaverCrawler
    from selenium_crawler import SeleniumNaverCrawler
    from mobile_crawler import MobileNaverCrawler
    from advanced_crawler import AdvancedNaverCrawler
    CRAWLERS_AVAILABLE = True
except ImportError:
    CRAWLERS_AVAILABLE = False
    print("âš ï¸  í¬ë¡¤ëŸ¬ ëª¨ë“ˆë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë™ì¼í•œ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

class SmartCrawlerScheduler:
    def __init__(self, config_file="crawler_config.json"):
        self.config_file = config_file
        self.config = self._load_config()
        self.setup_logging()
        self.running_jobs = {}
        
    def _load_config(self) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë˜ëŠ” ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        default_config = {
            "schedule": {
                "auto_run_times": ["02:00", "03:30", "05:00"],  # ìƒˆë²½ ì‹œê°„ëŒ€
                "timezone": "Asia/Seoul",
                "retry_interval_hours": 6
            },
            "vpn": {
                "enabled": False,
                "provider": "expressvpn",  # expressvpn, nordvpn, surfshark
                "countries": ["japan", "singapore", "australia"],  # í•œêµ­ ê·¼ì²˜ ì„œë²„
                "connect_command": "expressvpn connect {country}",
                "disconnect_command": "expressvpn disconnect",
                "status_command": "expressvpn status"
            },
            "crawlers": {
                "priority_order": ["stealth", "selenium", "mobile", "advanced"],
                "max_retries_per_crawler": 2,
                "delay_between_crawlers": 300  # 5ë¶„
            },
            "output": {
                "base_directory": "crawl_results",
                "filename_pattern": "{product_id}_{timestamp}_{crawler}.csv",
                "keep_logs_days": 30
            },
            "products": []  # í¬ë¡¤ë§í•  ìƒí’ˆ ëª©ë¡
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëˆ„ë½ëœ í‚¤ ë³´ì™„
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            except Exception as e:
                print(f"âš ï¸  ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return default_config
        else:
            # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
            self._save_config(default_config)
            return default_config
    
    def _save_config(self, config: Dict):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"crawler_scheduler_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def extract_product_id(self, url: str) -> Optional[str]:
        """URLì—ì„œ ìƒí’ˆ ID ìë™ ì¶”ì¶œ"""
        try:
            # ë‹¤ì–‘í•œ ë„¤ì´ë²„ ì‡¼í•‘ URL íŒ¨í„´ ì§€ì›
            patterns = [
                # ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì§ì ‘ ë§í¬
                r'smartstore\.naver\.com/[^/]+/products/(\d+)',
                r'smartstore\.naver\.com/.*?/(\d+)',
                
                # API ì—”ë“œí¬ì¸íŠ¸
                r'/products/(\d+)',
                r'/v1/products/(\d+)',
                
                # ì‡¼í•‘ ê²€ìƒ‰ ê²°ê³¼
                r'shopping\.naver\.com/.*?nvMid=(\d+)',
                r'shopping\.naver\.com/.*?productId=(\d+)',
                
                # ëª¨ë°”ì¼ ë§í¬
                r'm\.smartstore\.naver\.com/.*?/(\d+)',
                r'm\.shopping\.naver\.com/.*?nvMid=(\d+)',
                
                # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
                r'[?&]productId=(\d+)',
                r'[?&]nvMid=(\d+)',
                r'[?&]id=(\d+)',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    product_id = match.group(1)
                    self.logger.info(f"âœ… URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ ì„±ê³µ: {product_id}")
                    return product_id
            
            # ë§ˆì§€ë§‰ ì‹œë„: URL ëì˜ ìˆ«ì
            url_numbers = re.findall(r'/(\d{6,})', url)
            if url_numbers:
                product_id = url_numbers[-1]
                self.logger.info(f"âœ… URL íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìƒí’ˆ ID ì¶”ì¶œ: {product_id}")
                return product_id
            
            self.logger.error(f"âŒ URLì—ì„œ ìƒí’ˆ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ URL íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def add_product(self, url: str, name: str = "", priority: int = 1) -> bool:
        """ìƒí’ˆ ì¶”ê°€"""
        product_id = self.extract_product_id(url)
        if not product_id:
            return False
        
        product = {
            "id": product_id,
            "url": url,
            "name": name or f"ìƒí’ˆ_{product_id}",
            "priority": priority,
            "added_date": datetime.now().isoformat(),
            "last_crawl": None,
            "success_count": 0,
            "fail_count": 0,
            "enabled": True
        }
        
        # ì¤‘ë³µ ì²´í¬
        for existing in self.config["products"]:
            if existing["id"] == product_id:
                self.logger.info(f"âš ï¸  ìƒí’ˆ {product_id}ëŠ” ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return False
        
        self.config["products"].append(product)
        self._save_config(self.config)
        self.logger.info(f"âœ… ìƒí’ˆ ì¶”ê°€ ì™„ë£Œ: {name} ({product_id})")
        return True
    
    def remove_product(self, product_id: str) -> bool:
        """ìƒí’ˆ ì œê±°"""
        original_count = len(self.config["products"])
        self.config["products"] = [p for p in self.config["products"] if p["id"] != product_id]
        
        if len(self.config["products"]) < original_count:
            self._save_config(self.config)
            self.logger.info(f"âœ… ìƒí’ˆ {product_id} ì œê±° ì™„ë£Œ")
            return True
        else:
            self.logger.warning(f"âš ï¸  ìƒí’ˆ {product_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def list_products(self) -> List[Dict]:
        """ë“±ë¡ëœ ìƒí’ˆ ëª©ë¡ ë°˜í™˜"""
        return self.config["products"]
    
    def setup_vpn(self, provider: str, countries: List[str]):
        """VPN ì„¤ì •"""
        self.config["vpn"]["provider"] = provider
        self.config["vpn"]["countries"] = countries
        self.config["vpn"]["enabled"] = True
        
        # ì œê³µì—…ì²´ë³„ ëª…ë ¹ì–´ ì„¤ì •
        vpn_commands = {
            "expressvpn": {
                "connect": "expressvpn connect {country}",
                "disconnect": "expressvpn disconnect", 
                "status": "expressvpn status"
            },
            "nordvpn": {
                "connect": "nordvpn connect {country}",
                "disconnect": "nordvpn disconnect",
                "status": "nordvpn status"
            },
            "surfshark": {
                "connect": "surfshark-vpn attack {country}",
                "disconnect": "surfshark-vpn down",
                "status": "surfshark-vpn status"
            }
        }
        
        if provider in vpn_commands:
            self.config["vpn"].update(vpn_commands[provider])
        
        self._save_config(self.config)
        self.logger.info(f"âœ… VPN ì„¤ì • ì™„ë£Œ: {provider}")
    
    def connect_vpn(self) -> bool:
        """VPN ì—°ê²°"""
        if not self.config["vpn"]["enabled"]:
            return True
        
        try:
            # ëœë¤ êµ­ê°€ ì„ íƒ
            country = random.choice(self.config["vpn"]["countries"])
            command = self.config["vpn"]["connect_command"].format(country=country)
            
            self.logger.info(f"ğŸ”— VPN ì—°ê²° ì‹œë„: {country}")
            result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                self.logger.info(f"âœ… VPN ì—°ê²° ì„±ê³µ: {country}")
                time.sleep(10)  # ì—°ê²° ì•ˆì •í™” ëŒ€ê¸°
                return True
            else:
                self.logger.error(f"âŒ VPN ì—°ê²° ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ VPN ì—°ê²° ì˜¤ë¥˜: {e}")
            return False
    
    def disconnect_vpn(self) -> bool:
        """VPN ì—°ê²° í•´ì œ"""
        if not self.config["vpn"]["enabled"]:
            return True
        
        try:
            command = self.config["vpn"]["disconnect_command"]
            self.logger.info("ğŸ”Œ VPN ì—°ê²° í•´ì œ ì¤‘...")
            
            result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                self.logger.info("âœ… VPN ì—°ê²° í•´ì œ ì™„ë£Œ")
                return True
            else:
                self.logger.error(f"âŒ VPN í•´ì œ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ VPN í•´ì œ ì˜¤ë¥˜: {e}")
            return False
    
    def get_vpn_status(self) -> str:
        """VPN ìƒíƒœ í™•ì¸"""
        if not self.config["vpn"]["enabled"]:
            return "disabled"
        
        try:
            command = self.config["vpn"]["status_command"]
            result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10)
            return result.stdout.strip()
        except:
            return "unknown"
    
    def crawl_product(self, product: Dict) -> Optional[str]:
        """ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§"""
        product_id = product["id"]
        product_name = product["name"]
        
        self.logger.info(f"ğŸ¯ í¬ë¡¤ë§ ì‹œì‘: {product_name} ({product_id})")
        
        # VPN ì—°ê²°
        vpn_connected = self.connect_vpn()
        if not vpn_connected and self.config["vpn"]["enabled"]:
            self.logger.error("âŒ VPN ì—°ê²° ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨")
            return None
        
        success_file = None
        crawler_order = self.config["crawlers"]["priority_order"]
        
        try:
            for crawler_name in crawler_order:
                for retry in range(self.config["crawlers"]["max_retries_per_crawler"]):
                    try:
                        self.logger.info(f"ğŸ¤– {crawler_name} í¬ë¡¤ëŸ¬ ì‹œë„ {retry + 1}")
                        
                        # í¬ë¡¤ëŸ¬ ì‹¤í–‰
                        result = self._run_crawler(crawler_name, product_id)
                        
                        if result:
                            success_file = result
                            self.logger.info(f"âœ… {crawler_name} í¬ë¡¤ëŸ¬ë¡œ ì„±ê³µ!")
                            
                            # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
                            product["success_count"] = product.get("success_count", 0) + 1
                            product["last_crawl"] = datetime.now().isoformat()
                            self._save_config(self.config)
                            
                            break
                        else:
                            self.logger.warning(f"âš ï¸  {crawler_name} í¬ë¡¤ëŸ¬ ì‹¤íŒ¨")
                            time.sleep(30)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                            
                    except Exception as e:
                        self.logger.error(f"âŒ {crawler_name} í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {e}")
                
                if success_file:
                    break
                
                # ë‹¤ìŒ í¬ë¡¤ëŸ¬ ì‹œë„ ì „ ëŒ€ê¸°
                if crawler_name != crawler_order[-1]:
                    delay = self.config["crawlers"]["delay_between_crawlers"]
                    self.logger.info(f"â³ ë‹¤ìŒ í¬ë¡¤ëŸ¬ ì‹œë„ê¹Œì§€ {delay//60}ë¶„ ëŒ€ê¸°...")
                    time.sleep(delay)
            
            if not success_file:
                # ì‹¤íŒ¨ í†µê³„ ì—…ë°ì´íŠ¸
                product["fail_count"] = product.get("fail_count", 0) + 1
                self._save_config(self.config)
                self.logger.error(f"âŒ ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {product_name}")
            
        finally:
            # VPN ì—°ê²° í•´ì œ
            self.disconnect_vpn()
        
        return success_file
    
    def _run_crawler(self, crawler_name: str, product_id: str) -> Optional[str]:
        """ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        if not CRAWLERS_AVAILABLE:
            self.logger.error("âŒ í¬ë¡¤ëŸ¬ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        try:
            output_dir = Path(self.config["output"]["base_directory"])
            output_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_pattern = self.config["output"]["filename_pattern"]
            output_file = output_dir / filename_pattern.format(
                product_id=product_id,
                timestamp=timestamp,
                crawler=crawler_name
            )
            
            crawler = None
            result_df = None
            
            # í¬ë¡¤ëŸ¬ë³„ ì‹¤í–‰
            if crawler_name == "stealth":
                crawler = StealthNaverCrawler(product_id)
                result_df = crawler.crawl_reviews_stealth()
            elif crawler_name == "selenium":
                crawler = SeleniumNaverCrawler(product_id, headless=True)
                if crawler._setup_driver():
                    result_df = crawler.crawl_reviews()
                    crawler.close()
            elif crawler_name == "mobile":
                crawler = MobileNaverCrawler(product_id)
                result_df = crawler.crawl_reviews_mobile()
            elif crawler_name == "advanced":
                crawler = AdvancedNaverCrawler(product_id)
                result_df = crawler.crawl_reviews()
            
            # ê²°ê³¼ ì €ì¥
            if result_df is not None and not result_df.empty:
                result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
                return str(output_file)
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ {crawler_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None
    
    def crawl_all_products(self):
        """ëª¨ë“  í™œì„± ìƒí’ˆ í¬ë¡¤ë§"""
        active_products = [p for p in self.config["products"] if p.get("enabled", True)]
        
        if not active_products:
            self.logger.info("ğŸ“­ í¬ë¡¤ë§í•  í™œì„± ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        self.logger.info(f"ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘: {len(active_products)}ê°œ ìƒí’ˆ")
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        active_products.sort(key=lambda x: x.get("priority", 1), reverse=True)
        
        results = []
        for i, product in enumerate(active_products, 1):
            self.logger.info(f"ğŸ“¦ [{i}/{len(active_products)}] {product['name']}")
            
            result = self.crawl_product(product)
            results.append({
                "product": product["name"],
                "success": result is not None,
                "file": result
            })
            
            # ìƒí’ˆ ê°„ ëŒ€ê¸° (ë§ˆì§€ë§‰ ìƒí’ˆ ì œì™¸)
            if i < len(active_products):
                wait_time = random.randint(300, 900)  # 5-15ë¶„
                self.logger.info(f"â³ ë‹¤ìŒ ìƒí’ˆê¹Œì§€ {wait_time//60}ë¶„ ëŒ€ê¸°...")
                time.sleep(wait_time)
        
        # ê²°ê³¼ ìš”ì•½
        successful = sum(1 for r in results if r["success"])
        self.logger.info(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {successful}/{len(results)} ì„±ê³µ")
    
    def schedule_daily_crawling(self):
        """ì¼ì¼ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        for run_time in self.config["schedule"]["auto_run_times"]:
            schedule.every().day.at(run_time).do(self.crawl_all_products)
            self.logger.info(f"â° ìŠ¤ì¼€ì¤„ ë“±ë¡: ë§¤ì¼ {run_time}ì— í¬ë¡¤ë§ ì‹¤í–‰")
    
    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        self.schedule_daily_crawling()
        self.logger.info("ğŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - Ctrl+Cë¡œ ì¤‘ë‹¨")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ ì²´í¬
        except KeyboardInterrupt:
            self.logger.info("â›” ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ë‹¨ë¨")
    
    def manual_crawl(self, product_id_or_url: str) -> Optional[str]:
        """ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰"""
        # URLì¸ì§€ ìƒí’ˆ IDì¸ì§€ íŒë‹¨
        if product_id_or_url.startswith("http"):
            product_id = self.extract_product_id(product_id_or_url)
            if not product_id:
                self.logger.error("âŒ URLì—ì„œ ìƒí’ˆ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
        else:
            product_id = product_id_or_url
        
        # ì„ì‹œ ìƒí’ˆ ê°ì²´ ìƒì„±
        temp_product = {
            "id": product_id,
            "name": f"ìˆ˜ë™_í¬ë¡¤ë§_{product_id}",
            "url": product_id_or_url if product_id_or_url.startswith("http") else "",
            "priority": 1
        }
        
        return self.crawl_product(temp_product)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scheduler = SmartCrawlerScheduler()
    
    print("ğŸ¤– === ìŠ¤ë§ˆíŠ¸ ë„¤ì´ë²„ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ===")
    print("1. ìƒí’ˆ ì¶”ê°€")
    print("2. ìƒí’ˆ ëª©ë¡ ë³´ê¸°")
    print("3. ìƒí’ˆ ì œê±°")
    print("4. VPN ì„¤ì •")
    print("5. ìˆ˜ë™ í¬ë¡¤ë§")
    print("6. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    print("7. ì „ì²´ í¬ë¡¤ë§ (ì¦‰ì‹œ ì‹¤í–‰)")
    print("0. ì¢…ë£Œ")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (0-7): ").strip()
            
            if choice == "0":
                break
            elif choice == "1":
                url = input("ìƒí’ˆ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                name = input("ìƒí’ˆ ì´ë¦„ (ì„ íƒì‚¬í•­): ").strip()
                if scheduler.add_product(url, name):
                    print("âœ… ìƒí’ˆì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    print("âŒ ìƒí’ˆ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    
            elif choice == "2":
                products = scheduler.list_products()
                if products:
                    print("\nğŸ“¦ ë“±ë¡ëœ ìƒí’ˆ ëª©ë¡:")
                    for i, product in enumerate(products, 1):
                        status = "ğŸŸ¢" if product.get("enabled", True) else "ğŸ”´"
                        print(f"{i}. {status} {product['name']} ({product['id']})")
                        print(f"   ì„±ê³µ: {product.get('success_count', 0)}, ì‹¤íŒ¨: {product.get('fail_count', 0)}")
                else:
                    print("ğŸ“­ ë“±ë¡ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                    
            elif choice == "3":
                product_id = input("ì œê±°í•  ìƒí’ˆ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if scheduler.remove_product(product_id):
                    print("âœ… ìƒí’ˆì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    print("âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            elif choice == "4":
                print("\nì§€ì›ë˜ëŠ” VPN: expressvpn, nordvpn, surfshark")
                provider = input("VPN ì œê³µì—…ì²´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip().lower()
                countries_input = input("êµ­ê°€ ëª©ë¡ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: japan,singapore): ").strip()
                countries = [c.strip() for c in countries_input.split(",") if c.strip()]
                
                if provider and countries:
                    scheduler.setup_vpn(provider, countries)
                    print("âœ… VPN ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    print("âŒ ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    
            elif choice == "5":
                url_or_id = input("í¬ë¡¤ë§í•  ìƒí’ˆ URL ë˜ëŠ” IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                print("ğŸš€ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                result = scheduler.manual_crawl(url_or_id)
                if result:
                    print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ! ê²°ê³¼: {result}")
                else:
                    print("âŒ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    
            elif choice == "6":
                print("â° ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                scheduler.start_scheduler()
                
            elif choice == "7":
                print("ğŸš€ ì „ì²´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                scheduler.crawl_all_products()
                
            else:
                print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
        except KeyboardInterrupt:
            print("\nâ›” í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()