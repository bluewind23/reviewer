"""
ê³ ê¸‰ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬
IP ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì„ í¬í•¨
"""

import requests
import pandas as pd
import json
import time
import random
import hashlib
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib3.exceptions import InsecureRequestWarning
# ë¶„ì„ ëª¨ë“ˆ import
from analysis import analyze_sentiment, topic_modeling

# SSL ê²½ê³  ë¹„í™œì„±í™”
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# --- ì„¤ì • ë¶€ë¶„ ---
PRODUCT_ID = "5753732771"
OUTPUT_FILE_NAME = "reviews.csv"
NUM_TOPICS = 5

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Linux; Android 13; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36"
]

PROXIES = []

class AdvancedNaverCrawler:
    def __init__(self, product_id):
        self.product_id = product_id
        self.session = self._create_session()
        self.request_count = 0
        self.last_request_time = 0
        
    def _create_session(self):
        session = requests.Session()
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504, 403],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.verify = False
        return session
    
    def _get_dynamic_headers(self, referer_url=None):
        user_agent = random.choice(USER_AGENTS)
        headers = {
            "User-Agent": user_agent,
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }
        
        if referer_url:
            headers["Referer"] = referer_url
        
        if "Chrome" in user_agent and "Edge" not in user_agent:
            headers["sec-ch-ua"] = '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"'
            headers["sec-ch-ua-mobile"] = "?0"
            headers["sec-ch-ua-platform"] = '"Windows"' if "Windows" in user_agent else '"macOS"'
        
        if "Firefox" in user_agent:
            headers["DNT"] = "1"
            headers["Upgrade-Insecure-Requests"] = "1"
        
        if random.random() > 0.5:
            headers["X-Requested-With"] = "XMLHttpRequest"
        
        if random.random() > 0.7:
            headers["Origin"] = "https://smartstore.naver.com"
            
        return headers
    
    def _intelligent_delay(self):
        current_time = time.time()
        if self.last_request_time > 0:
            elapsed = current_time - self.last_request_time
            if self.request_count > 10:
                min_delay = 3
                max_delay = 8
            elif self.request_count > 5:
                min_delay = 2
                max_delay = 5
            else:
                min_delay = 1
                max_delay = 3
            
            if elapsed < min_delay:
                additional_delay = min_delay - elapsed + random.uniform(0, max_delay - min_delay)
                print(f"ì§€ì—° ì‹œê°„: {additional_delay:.2f}ì´ˆ")
                time.sleep(additional_delay)
        
        self.last_request_time = time.time()
        self.request_count += 1
    
    def _generate_session_fingerprint(self):
        fingerprint_urls = [
            "https://smartstore.naver.com/",
            f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
        ]
        
        for url in fingerprint_urls:
            try:
                headers = self._get_dynamic_headers()
                response = self.session.get(url, headers=headers, timeout=10)
                if response.status_code == 200:
                    print(f"í•‘ê±°í”„ë¦°íŠ¸ ìƒì„±: {url}")
                time.sleep(random.uniform(0.5, 2))
            except:
                continue
    
    def get_product_info(self):
        print("ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        self._generate_session_fingerprint()
        info_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}/summary"
        
        for attempt in range(5):
            try:
                self._intelligent_delay()
                headers = self._get_dynamic_headers(referer_url=f"https://smartstore.naver.com/i/v1/products/{self.product_id}")
                proxy = random.choice(PROXIES) if PROXIES else None
                response = self.session.get(info_url, headers=headers, proxies=proxy, timeout=15, allow_redirects=True)
                
                if response.status_code == 200:
                    try:
                        product_info = response.json()
                        merchant_no = product_info['product']['channel']['channelNo']
                        origin_product_no = product_info['product']['productNo']
                        print(f"âœ… ìƒí’ˆ ì •ë³´ íšë“: Merchant No: {merchant_no}, Origin Product No: {origin_product_no}")
                        return merchant_no, origin_product_no
                    except (json.JSONDecodeError, KeyError) as e:
                        print(f"âŒ ìƒí’ˆ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        return None, None
                        
                elif response.status_code == 429:
                    delay = min(60 * (2 ** attempt), 300)
                    print(f"âš ï¸  Rate limit ê°ì§€. {delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                    
                elif response.status_code == 403:
                    delay = min(120 * (attempt + 1), 600)
                    print(f"ğŸš« ì ‘ê·¼ ê±°ë¶€ë¨. {delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                    
                else:
                    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœ ì½”ë“œ: {response.status_code}")
                    if attempt < 4:
                        time.sleep(random.uniform(10, 30))
                        
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < 4:
                    time.sleep(random.uniform(10, 30))
        
        print("âŒ ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None, None
    
    def crawl_reviews(self):
        merchant_no, origin_product_no = self.get_product_info()
        
        if not merchant_no or not origin_product_no:
            return None
            
        all_reviews = []
        page = 1
        consecutive_failures = 0
        max_consecutive_failures = 3
        
        print("ë¦¬ë·° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        while True:
            url = (
                f"https://smartstore.naver.com/main/products/{origin_product_no}/reviews/writable-reviews"
                f"?page={page}&sort=REVIEW_RANKING"
                f"&merchantNo={merchant_no}"
            )
            
            try:
                self._intelligent_delay()
                headers = self._get_dynamic_headers(referer_url=f"https://smartstore.naver.com/i/v1/products/{self.product_id}")
                proxy = random.choice(PROXIES) if PROXIES else None
                response = self.session.get(url, headers=headers, proxies=proxy, timeout=20)
                
                if response.status_code == 200:
                    consecutive_failures = 0
                    try:
                        data = response.json()
                        reviews = data.get('contents', [])
                        
                        if not reviews:
                            print("âœ… ëª¨ë“  ë¦¬ë·°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                            break
                        
                        for review in reviews:
                            option_text = " / ".join([opt['optionContent'] for opt in review.get('productOptionContents', []) if 'optionContent' in opt])
                            all_reviews.append({
                                'id': review.get('id'),
                                'rating': review.get('reviewScore'),
                                'writer': review.get('writerMemberId'),
                                'date': review.get('createDate'),
                                'content': review.get('reviewContent', ''),
                                'option': option_text,
                            })
                        
                        print(f"ğŸ“„ {page} í˜ì´ì§€: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(all_reviews)}ê°œ)")
                        page += 1
                        
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        consecutive_failures += 1
                        
                elif response.status_code == 429:
                    print("âš ï¸  Rate limit ê°ì§€. ê¸´ ëŒ€ê¸° ì‹œê°„ ì ìš©...")
                    time.sleep(random.uniform(120, 180))
                    consecutive_failures += 1
                    
                elif response.status_code == 403:
                    print("ğŸš« ì ‘ê·¼ ê±°ë¶€. IP ì°¨ë‹¨ ê°€ëŠ¥ì„± ë†’ìŒ.")
                    time.sleep(random.uniform(300, 600))
                    consecutive_failures += 1
                    
                else:
                    print(f"âŒ HTTP {response.status_code} ì˜¤ë¥˜")
                    consecutive_failures += 1
                    time.sleep(random.uniform(30, 60))
                
                if consecutive_failures >= max_consecutive_failures:
                    print(f"âŒ {max_consecutive_failures}íšŒ ì—°ì† ì‹¤íŒ¨ë¡œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    break
                time.sleep(random.uniform(60, 120))
        
        if all_reviews:
            print(f"âœ… ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame(all_reviews)
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

if __name__ == '__main__':
    print("ğŸš€ ê³ ê¸‰ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"ğŸ“¦ ë¶„ì„ ëŒ€ìƒ ìƒí’ˆ ID: {PRODUCT_ID}")
    
    crawler = AdvancedNaverCrawler(PRODUCT_ID)
    review_df = crawler.crawl_reviews()
    
    if review_df is not None and not review_df.empty:
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        positive_keywords = ['ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ë¹ ë¥¸', 'í¸í•˜ê³ ', 'ì˜ˆë»ìš”', 'ê°€ë³ê³ ', 'íŠ¼íŠ¼', 'ì˜', 'ë§˜ì—']
        negative_keywords = ['ë¶ˆí¸', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬', 'ë¶ˆë§Œ', 'ëŠë¦°', 'ë¬´ê±°', 'ì•½í•œ', 'ë¬¸ì œ', 'ê³ ì¥']
        
        review_df['sentiment'] = review_df['content'].apply(lambda x: analyze_sentiment(x, positive_keywords, negative_keywords))
        
        print("\n--- ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼ ---")
        print(review_df['sentiment'].value_counts())
        
        print("\n--- ğŸ·ï¸  í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---")
        review_df = topic_modeling(review_df, NUM_TOPICS)
        
        review_df.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\n--- ğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ---")
        print(review_df.head())
        
        print(f"\n--- ğŸ“Š ìˆ˜ì§‘ í†µê³„ ---")
        print(f"ì´ ë¦¬ë·° ìˆ˜: {len(review_df)}")
        print(f"í‰ê·  í‰ì : {review_df['rating'].mean():.2f}")
        print(f"ê¸ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ê¸ì •'])}")
        print(f"ë¶€ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ë¶€ì •'])}")
        print(f"ì¤‘ë¦½ ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ì¤‘ë¦½'])}")
        
    else:
        print("âŒ í¬ë¡¤ë§ëœ ë¦¬ë·°ê°€ ì—†ì–´ ë¶„ì„ì„ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. ìƒí’ˆ IDê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸")
        print("3. í”„ë¡ì‹œ ì„¤ì • ì¶”ê°€ ê³ ë ¤")
        print("4. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ ì„¤ì •")