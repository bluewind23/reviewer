"""
ê³ ê¸‰ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬
IP ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì„ í¬í•¨
"""

import requests
import pandas as pd
import json
import time
import random
import hashlib
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib3.exceptions import InsecureRequestWarning
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# SSL ê²½ê³  ë¹„í™œì„±í™”
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# --- ì„¤ì • ë¶€ë¶„ ---
PRODUCT_ID = "5753732771"  # ë¶„ì„í•˜ê³  ì‹¶ì€ ìƒí’ˆì˜ ID
OUTPUT_FILE_NAME = "reviews.csv"
NUM_TOPICS = 5 # ë¶„ë¥˜í•  ì£¼ì œ(í† í”½)ì˜ ê°œìˆ˜

# ë” ë‹¤ì–‘í•œ User-Agent ëª©ë¡ (ì‹¤ì œ ë¸Œë¼ìš°ì € í†µê³„ ê¸°ë°˜)
USER_AGENTS = [
    # Chrome on Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    
    # Chrome on macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    
    # Safari on macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    
    # Firefox
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/119.0",
    
    # Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    
    # Mobile User Agents
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Linux; Android 13; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36"
]

# í”„ë¡ì‹œ ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
PROXIES = [
    # ë¬´ë£Œ í”„ë¡ì‹œ ì„œë¹„ìŠ¤ë“¤ (ì‹¤ì œ ì‚¬ìš©ì‹œ ê²€ì¦ í•„ìš”)
    # {'http': 'http://proxy1:port', 'https': 'https://proxy1:port'},
    # ì‹¤ì œ í”„ë¡ì‹œ ì„œë²„ê°€ ìˆì„ ê²½ìš° ì—¬ê¸°ì— ì¶”ê°€
]

class AdvancedNaverCrawler:
    def __init__(self, product_id):
        self.product_id = product_id
        self.session = self._create_session()
        self.request_count = 0
        self.last_request_time = 0
        
    def _create_session(self):
        """ê³ ê¸‰ ì„¸ì…˜ ì„¤ì •"""
        session = requests.Session()
        
        # ì¬ì‹œë„ ì „ëµ
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504, 403],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # ê¸°ë³¸ ì„¸ì…˜ ì„¤ì •
        session.verify = False  # SSL ê²€ì¦ ë¹„í™œì„±í™” (í•„ìš”ì‹œ)
        
        return session
    
    def _get_dynamic_headers(self, referer_url=None):
        """ë™ì  í—¤ë” ìƒì„±"""
        user_agent = random.choice(USER_AGENTS)
        
        # ë¸Œë¼ìš°ì €ë³„ í—¤ë” ì°¨ì´ì  êµ¬í˜„
        headers = {
            "User-Agent": user_agent,
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        }
        
        if referer_url:
            headers["Referer"] = referer_url
        
        # Chrome ê¸°ë°˜ ë¸Œë¼ìš°ì €ì¸ ê²½ìš°
        if "Chrome" in user_agent and "Edge" not in user_agent:
            headers["sec-ch-ua"] = '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"'
            headers["sec-ch-ua-mobile"] = "?0"
            headers["sec-ch-ua-platform"] = '"Windows"' if "Windows" in user_agent else '"macOS"'
        
        # Firefoxì¸ ê²½ìš°
        if "Firefox" in user_agent:
            headers["DNT"] = "1"
            headers["Upgrade-Insecure-Requests"] = "1"
        
        # ëœë¤í•˜ê²Œ ì¼ë¶€ í—¤ë” ì¶”ê°€/ì œê±°
        if random.random() > 0.5:
            headers["X-Requested-With"] = "XMLHttpRequest"
        
        if random.random() > 0.7:
            headers["Origin"] = "https://smartstore.naver.com"
            
        return headers
    
    def _intelligent_delay(self):
        """ì§€ëŠ¥ì  ì§€ì—° ì‹œìŠ¤í…œ"""
        current_time = time.time()
        
        # ì´ì „ ìš”ì²­ìœ¼ë¡œë¶€í„°ì˜ ì‹œê°„ ê°„ê²© ê³„ì‚°
        if self.last_request_time > 0:
            elapsed = current_time - self.last_request_time
            
            # ìš”ì²­ ë¹ˆë„ì— ë”°ë¥¸ ë™ì  ì§€ì—°
            if self.request_count > 10:  # ë§ì€ ìš”ì²­ í›„
                min_delay = 3
                max_delay = 8
            elif self.request_count > 5:  # ì ë‹¹í•œ ìš”ì²­ í›„
                min_delay = 2
                max_delay = 5
            else:  # ì´ˆê¸° ìš”ì²­
                min_delay = 1
                max_delay = 3
            
            # ìµœì†Œ ê°„ê²© ë³´ì¥
            if elapsed < min_delay:
                additional_delay = min_delay - elapsed + random.uniform(0, max_delay - min_delay)
                print(f"ì§€ì—° ì‹œê°„: {additional_delay:.2f}ì´ˆ")
                time.sleep(additional_delay)
        
        self.last_request_time = time.time()
        self.request_count += 1
    
    def _generate_session_fingerprint(self):
        """ì„¸ì…˜ í•‘ê±°í”„ë¦°íŠ¸ ìƒì„±"""
        # ë¸Œë¼ìš°ì € ì„¸ì…˜ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•œ ì¶”ê°€ ìš”ì²­ë“¤
        fingerprint_urls = [
            "https://smartstore.naver.com/",
            f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
        ]
        
        for url in fingerprint_urls:
            try:
                headers = self._get_dynamic_headers()
                response = self.session.get(url, headers=headers, timeout=10)
                if response.status_code == 200:
                    print(f"í•‘ê±°í”„ë¦°íŠ¸ ìƒì„±: {url}")
                time.sleep(random.uniform(0.5, 2))
            except:
                continue
    
    def get_product_info(self):
        """ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í–¥ìƒëœ ë²„ì „)"""
        print("ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        # ì„¸ì…˜ í•‘ê±°í”„ë¦°íŠ¸ ìƒì„±
        self._generate_session_fingerprint()
        
        info_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}/summary"
        
        for attempt in range(5):  # ìµœëŒ€ 5ë²ˆ ì‹œë„
            try:
                self._intelligent_delay()
                
                headers = self._get_dynamic_headers(
                    referer_url=f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
                )
                
                # í”„ë¡ì‹œ ì‚¬ìš© (ì„¤ì •ëœ ê²½ìš°)
                proxy = random.choice(PROXIES) if PROXIES else None
                
                response = self.session.get(
                    info_url,
                    headers=headers,
                    proxies=proxy,
                    timeout=15,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    try:
                        product_info = response.json()
                        merchant_no = product_info['product']['channel']['channelNo']
                        origin_product_no = product_info['product']['productNo']
                        print(f"âœ… ìƒí’ˆ ì •ë³´ íšë“: Merchant No: {merchant_no}, Origin Product No: {origin_product_no}")
                        return merchant_no, origin_product_no
                    except (json.JSONDecodeError, KeyError) as e:
                        print(f"âŒ ìƒí’ˆ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        return None, None
                        
                elif response.status_code == 429:
                    delay = min(60 * (2 ** attempt), 300)  # ì§€ìˆ˜ ë°±ì˜¤í”„, ìµœëŒ€ 5ë¶„
                    print(f"âš ï¸  Rate limit ê°ì§€. {delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                    
                elif response.status_code == 403:
                    delay = min(120 * (attempt + 1), 600)  # ìµœëŒ€ 10ë¶„
                    print(f"ğŸš« ì ‘ê·¼ ê±°ë¶€ë¨. {delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                    
                else:
                    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœ ì½”ë“œ: {response.status_code}")
                    if attempt < 4:
                        time.sleep(random.uniform(10, 30))
                        
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < 4:
                    time.sleep(random.uniform(10, 30))
        
        print("âŒ ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None, None
    
    def crawl_reviews(self):
        """ë¦¬ë·° í¬ë¡¤ë§ (í–¥ìƒëœ ë²„ì „)"""
        merchant_no, origin_product_no = self.get_product_info()
        
        if not merchant_no or not origin_product_no:
            return None
            
        all_reviews = []
        page = 1
        consecutive_failures = 0
        max_consecutive_failures = 3
        
        print("ë¦¬ë·° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        while True:
            url = (
                f"https://smartstore.naver.com/main/products/{origin_product_no}/reviews/writable-reviews"
                f"?page={page}&sort=REVIEW_RANKING"
                f"&merchantNo={merchant_no}"
            )
            
            try:
                self._intelligent_delay()
                
                headers = self._get_dynamic_headers(
                    referer_url=f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
                )
                
                # í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
                proxy = random.choice(PROXIES) if PROXIES else None
                
                response = self.session.get(
                    url,
                    headers=headers,
                    proxies=proxy,
                    timeout=20
                )
                
                if response.status_code == 200:
                    consecutive_failures = 0  # ì„±ê³µì‹œ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                    
                    try:
                        data = response.json()
                        reviews = data.get('contents', [])
                        
                        if not reviews:
                            print("âœ… ëª¨ë“  ë¦¬ë·°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                            break
                        
                        for review in reviews:
                            option_text = " / ".join([
                                opt['optionContent'] 
                                for opt in review.get('productOptionContents', []) 
                                if 'optionContent' in opt
                            ])
                            
                            all_reviews.append({
                                'id': review.get('id'),
                                'rating': review.get('reviewScore'),
                                'writer': review.get('writerMemberId'),
                                'date': review.get('createDate'),
                                'content': review.get('reviewContent', ''),
                                'option': option_text,
                            })
                        
                        print(f"ğŸ“„ {page} í˜ì´ì§€: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(all_reviews)}ê°œ)")
                        page += 1
                        
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        consecutive_failures += 1
                        
                elif response.status_code == 429:
                    print("âš ï¸  Rate limit ê°ì§€. ê¸´ ëŒ€ê¸° ì‹œê°„ ì ìš©...")
                    time.sleep(random.uniform(120, 180))  # 2-3ë¶„ ëŒ€ê¸°
                    consecutive_failures += 1
                    
                elif response.status_code == 403:
                    print("ğŸš« ì ‘ê·¼ ê±°ë¶€. IP ì°¨ë‹¨ ê°€ëŠ¥ì„± ë†’ìŒ.")
                    time.sleep(random.uniform(300, 600))  # 5-10ë¶„ ëŒ€ê¸°
                    consecutive_failures += 1
                    
                else:
                    print(f"âŒ HTTP {response.status_code} ì˜¤ë¥˜")
                    consecutive_failures += 1
                    time.sleep(random.uniform(30, 60))
                
                # ì—°ì† ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                if consecutive_failures >= max_consecutive_failures:
                    print(f"âŒ {max_consecutive_failures}íšŒ ì—°ì† ì‹¤íŒ¨ë¡œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    break
                time.sleep(random.uniform(60, 120))
        
        if all_reviews:
            print(f"âœ… ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame(all_reviews)
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

def analyze_sentiment(text, positive_keywords, negative_keywords):
    """ê°ì„± ë¶„ì„ (ê¸°ì¡´ê³¼ ë™ì¼)"""
    score = 0
    for keyword in positive_keywords:
        if keyword in text:
            score += 1
    for keyword in negative_keywords:
        if keyword in text:
            score -= 1
    
    if score > 0:
        return 'ê¸ì •'
    elif score < 0:
        return 'ë¶€ì •'
    else:
        return 'ì¤‘ë¦½'

def topic_modeling(df, num_topics):
    """í† í”½ ëª¨ë¸ë§ (ê¸°ì¡´ê³¼ ë™ì¼)"""
    okt = Okt()
    
    def tokenize(text):
        if isinstance(text, str):
            return [token for token in okt.nouns(text) if len(token) > 1]
        return []

    df['tokens'] = df['content'].apply(tokenize)
    
    texts = [" ".join(tokens) for tokens in df['tokens']]

    if not any(texts):
        print("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í† í”½ ëª¨ë¸ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        df['topic'] = 'ë¶„ì„ ë¶ˆê°€'
        return df

    vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2)
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(tfidf_matrix)
    
    topic_results = lda.transform(tfidf_matrix)
    df['topic'] = topic_results.argmax(axis=1)

    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(lda.components_):
        top_keywords = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]
        print(f"í† í”½ #{topic_idx}: {', '.join(top_keywords)}")
        
    return df

if __name__ == '__main__':
    print("ğŸš€ ê³ ê¸‰ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"ğŸ“¦ ë¶„ì„ ëŒ€ìƒ ìƒí’ˆ ID: {PRODUCT_ID}")
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = AdvancedNaverCrawler(PRODUCT_ID)
    
    # ë¦¬ë·° í¬ë¡¤ë§
    review_df = crawler.crawl_reviews()
    
    if review_df is not None and not review_df.empty:
        print("\nğŸ“Š ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ê°ì„± ë¶„ì„ í‚¤ì›Œë“œ
        positive_keywords = ['ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ë¹ ë¥¸', 'í¸í•˜ê³ ', 'ì˜ˆë»ìš”', 'ê°€ë³ê³ ', 'íŠ¼íŠ¼', 'ì˜', 'ë§˜ì—']
        negative_keywords = ['ë¶ˆí¸', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬', 'ë¶ˆë§Œ', 'ëŠë¦°', 'ë¬´ê±°', 'ì•½í•œ', 'ë¬¸ì œ', 'ê³ ì¥']
        
        # ê°ì„± ë¶„ì„
        review_df['sentiment'] = review_df['content'].apply(
            lambda x: analyze_sentiment(x, positive_keywords, negative_keywords)
        )
        
        print("\n--- ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼ ---")
        print(review_df['sentiment'].value_counts())
        
        # í† í”½ ëª¨ë¸ë§
        print("\n--- ğŸ·ï¸  í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---")
        review_df = topic_modeling(review_df, NUM_TOPICS)
        
        # ê²°ê³¼ ì €ì¥
        review_df.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\n--- ğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ---")
        print(review_df.head())
        
        # í†µê³„ ì •ë³´
        print(f"\n--- ğŸ“Š ìˆ˜ì§‘ í†µê³„ ---")
        print(f"ì´ ë¦¬ë·° ìˆ˜: {len(review_df)}")
        print(f"í‰ê·  í‰ì : {review_df['rating'].mean():.2f}")
        print(f"ê¸ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ê¸ì •'])}")
        print(f"ë¶€ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ë¶€ì •'])}")
        print(f"ì¤‘ë¦½ ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ì¤‘ë¦½'])}")
        
    else:
        print("âŒ í¬ë¡¤ë§ëœ ë¦¬ë·°ê°€ ì—†ì–´ ë¶„ì„ì„ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. ìƒí’ˆ IDê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸")
        print("3. í”„ë¡ì‹œ ì„¤ì • ì¶”ê°€ ê³ ë ¤")
        print("4. ë” ê¸´ ëŒ€ê¸° ì‹œê°„ ì„¤ì •")