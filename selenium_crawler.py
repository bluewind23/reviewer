"""
Selenium ê¸°ë°˜ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬
ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ IP ì°¨ë‹¨ì„ ìš°íšŒ
"""

import pandas as pd
import json
import time
import random
import re
from datetime import datetime
# ë¶„ì„ ëª¨ë“ˆ import
from analysis import analyze_sentiment, topic_modeling

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.action_chains import ActionChains
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸  Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install selenium")
    print("Chrome ë“œë¼ì´ë²„ë„ í•„ìš”í•©ë‹ˆë‹¤: https://chromedriver.chromium.org/")

# --- ì„¤ì • ë¶€ë¶„ ---
PRODUCT_ID = "5753732771"
OUTPUT_FILE_NAME = "reviews_selenium.csv"
NUM_TOPICS = 5

class SeleniumNaverCrawler:
    def __init__(self, product_id, headless=True):
        if not SELENIUM_AVAILABLE:
            raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.product_id = product_id
        self.headless = headless
        self.driver = None
        self.wait = None
        
    def _setup_driver(self):
        chrome_options = Options()
        if self.headless: chrome_options.add_argument("--headless")
        
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]
        chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        window_sizes = ["1920,1080", "1366,768", "1440,900", "1536,864"]
        chrome_options.add_argument(f"--window-size={random.choice(window_sizes)}")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.wait = WebDriverWait(self.driver, 30)
            print("âœ… Chrome ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("Chrome ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return False
    
    def _human_like_scroll(self):
        scroll_patterns = [
            lambda: self.driver.execute_script("window.scrollBy(0, 300);"),
            lambda: self.driver.execute_script("window.scrollBy(0, 800);"),
            lambda: self.driver.execute_script("window.scrollBy(0, -200);"),
            lambda: self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"),
        ]
        for _ in range(random.randint(2, 5)):
            random.choice(scroll_patterns)()
            time.sleep(random.uniform(0.5, 2.0))
    
    def _human_like_delay(self):
        delay = random.uniform(2, 5)
        print(f"â³ {delay:.2f}ì´ˆ ëŒ€ê¸°...")
        time.sleep(delay)
    
    def _simulate_human_activity(self):
        activities = [self._random_mouse_move, self._random_scroll, self._pause_and_read]
        if random.random() < 0.3:
            try: random.choice(activities)()
            except: pass
    
    def _random_mouse_move(self):
        try:
            actions = ActionChains(self.driver)
            actions.move_by_offset(random.randint(-100, 100), random.randint(-100, 100)).perform()
            time.sleep(random.uniform(0.1, 0.5))
        except: pass
    
    def _random_scroll(self):
        self.driver.execute_script(f"window.scrollBy(0, {random.randint(100, 500)});")
        time.sleep(random.uniform(0.5, 1.5))
    
    def _pause_and_read(self):
        time.sleep(random.uniform(3, 8))
    
    def get_product_info(self):
        print("ğŸ” ë¸Œë¼ìš°ì €ë¡œ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        try:
            product_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
            self.driver.get(product_url)
            self._human_like_delay()
            self._simulate_human_activity()
            
            merchant_no, origin_product_no = None, None
            
            try:
                page_source = self.driver.page_source
                json_pattern = r'"productNo":\s*"?(\d+)"?'
                match = re.search(json_pattern, page_source)
                if match: origin_product_no = match.group(1)
                
                merchant_pattern = r'"channelNo":\s*"?(\d+)"?'
                match = re.search(merchant_pattern, page_source)
                if match: merchant_no = match.group(1)
                
                if merchant_no and origin_product_no:
                    print(f"âœ… ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì„±ê³µ!")
                    print(f"   ğŸ“Š Merchant No: {merchant_no}")
                    print(f"   ğŸ“Š Origin Product No: {origin_product_no}")
                    return merchant_no, origin_product_no
            except Exception as e:
                print(f"âš ï¸  ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            
            try:
                api_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}/summary"
                self.driver.get(api_url)
                time.sleep(3)
                body_text = self.driver.find_element(By.TAG_NAME, "body").text
                
                if body_text and body_text.strip().startswith('{'):
                    data = json.loads(body_text)
                    merchant_no = data['product']['channel']['channelNo']
                    origin_product_no = data['product']['productNo']
                    print(f"âœ… APIë¡œ ìƒí’ˆ ì •ë³´ íšë“!")
                    print(f"   ğŸ“Š Merchant No: {merchant_no}")
                    print(f"   ğŸ“Š Origin Product No: {origin_product_no}")
                    return merchant_no, origin_product_no
            except Exception as e:
                print(f"âš ï¸  API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            print("âŒ ìƒí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None, None
        except Exception as e:
            print(f"âŒ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None, None
    
    def crawl_reviews(self):
        merchant_no, origin_product_no = self.get_product_info()
        if not merchant_no or not origin_product_no: return None
        
        all_reviews, page, max_pages = [], 1, 100
        print("ğŸ“ ë¸Œë¼ìš°ì €ë¡œ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
        
        while page <= max_pages:
            try:
                review_url = (f"https://smartstore.naver.com/main/products/{origin_product_no}/reviews/writable-reviews?page={page}&sort=REVIEW_RANKING&merchantNo={merchant_no}")
                print(f"ğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
                self.driver.get(review_url)
                self._human_like_delay()
                
                try:
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text
                    if not body_text or not body_text.strip().startswith('{'):
                        print(f"âŒ í˜ì´ì§€ {page}: ì˜¬ë°”ë¥¸ JSON ì‘ë‹µì´ ì•„ë‹™ë‹ˆë‹¤")
                        break
                    
                    data = json.loads(body_text)
                    reviews = data.get('contents', [])
                    if not reviews:
                        print("âœ… ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                        break
                    
                    for review in reviews:
                        option_text = " / ".join([opt['optionContent'] for opt in review.get('productOptionContents', []) if 'optionContent' in opt])
                        all_reviews.append({
                            'id': review.get('id'),
                            'rating': review.get('reviewScore'),
                            'writer': review.get('writerMemberId'),
                            'date': review.get('createDate'),
                            'content': review.get('reviewContent', ''),
                            'option': option_text,
                        })
                    
                    print(f"âœ… í˜ì´ì§€ {page}: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ (ì´ {len(all_reviews)}ê°œ)")
                    page += 1
                    self._simulate_human_activity()
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}\nì‘ë‹µ ë‚´ìš©: {body_text[:200]}...")
                    break
                except Exception as e:
                    print(f"âŒ ë¦¬ë·° íŒŒì‹± ì˜¤ë¥˜: {e}"); break
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"); break
        
        if all_reviews:
            print(f"ğŸ‰ ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            return pd.DataFrame(all_reviews)
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
    
    def close(self):
        if self.driver: self.driver.quit(); print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")

if __name__ == '__main__':
    if not SELENIUM_AVAILABLE:
        print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit(1)
    
    print("ğŸ¤– === Selenium ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ ì‹œì‘ ===")
    print(f"ğŸ¯ íƒ€ê²Ÿ ìƒí’ˆ: {PRODUCT_ID}")
    print("ğŸ”§ Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
    
    crawler = None
    try:
        crawler = SeleniumNaverCrawler(PRODUCT_ID, headless=True)
        if not crawler._setup_driver():
            print("âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨")
            exit(1)
        
        review_df = crawler.crawl_reviews()
        
        if review_df is not None and not review_df.empty:
            print("\nğŸ“Š === ë°ì´í„° ë¶„ì„ ì‹œì‘ ===")
            positive_keywords = ['ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ë¹ ë¥¸', 'í¸í•˜ê³ ', 'ì˜ˆë»ìš”', 'ê°€ë³ê³ ', 'íŠ¼íŠ¼', 'ì˜', 'ë§˜ì—']
            negative_keywords = ['ë¶ˆí¸', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬', 'ë¶ˆë§Œ', 'ëŠë¦°', 'ë¬´ê±°', 'ì•½í•œ', 'ë¬¸ì œ', 'ê³ ì¥']
            
            review_df['sentiment'] = review_df['content'].apply(lambda x: analyze_sentiment(x, positive_keywords, negative_keywords))
            
            print("\n--- ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼ ---")
            print(review_df['sentiment'].value_counts())
            
            print("\n--- ğŸ·ï¸  í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---")
            review_df = topic_modeling(review_df, NUM_TOPICS)
            
            review_df.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            print(f"\n--- ğŸ“Š ìˆ˜ì§‘ í†µê³„ ---")
            print(f"ğŸ“ ì´ ë¦¬ë·° ìˆ˜: {len(review_df)}")
            print(f"â­ í‰ê·  í‰ì : {review_df['rating'].mean():.2f}")
            print(f"ğŸ˜Š ê¸ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ê¸ì •'])}")
            print(f"ğŸ˜ ë¶€ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ë¶€ì •'])}")
            print(f"ğŸ˜ ì¤‘ë¦½ ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ì¤‘ë¦½'])}")
        else:
            print("\nâŒ === í¬ë¡¤ë§ ì‹¤íŒ¨ ===")
    except KeyboardInterrupt:
        print("\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if crawler: crawler.close()