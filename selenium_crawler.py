"""
Selenium ê¸°ë°˜ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬
ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ IP ì°¨ë‹¨ì„ ìš°íšŒ
"""
import pandas as pd
import json
import time
import random
from datetime import datetime
from analysis import analyze_sentiment, topic_modeling

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.action_chains import ActionChains
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸  Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install selenium")
    print("Chrome ë“œë¼ì´ë²„ë„ í•„ìš”í•©ë‹ˆë‹¤: https://chromedriver.chromium.org/")

# --- ì„¤ì • ë¶€ë¶„ ---
PRODUCT_ID = "5753732771"
OUTPUT_FILE_NAME = "reviews_selenium.csv"
NUM_TOPICS = 5

class SeleniumNaverCrawler:
    def __init__(self, product_id, headless=True):
        if not SELENIUM_AVAILABLE:
            raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.product_id = product_id
        self.headless = headless
        self.driver = None
        self.wait = None
        
    def _setup_driver(self):
        """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        ]
        chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        window_sizes = ["1920,1080", "1366,768", "1440,900"]
        chrome_options.add_argument(f"--window-size={random.choice(window_sizes)}")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.wait = WebDriverWait(self.driver, 30)
            print("âœ… Chrome ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("Chrome ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  PATHì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return False
    
    def _human_like_delay(self, min_delay=2, max_delay=5):
        """ì¸ê°„ê³¼ ê°™ì€ ì§€ì—°"""
        delay = random.uniform(min_delay, max_delay)
        print(f"â³ {delay:.2f}ì´ˆ ëŒ€ê¸°...")
        time.sleep(delay)

    def get_product_info(self):
        """ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        print("ğŸ” ë¸Œë¼ìš°ì €ë¡œ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        api_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}/summary"
        try:
            self.driver.get(api_url)
            self._human_like_delay()
            
            body_text = self.driver.find_element(By.TAG_NAME, "body").text
            if not body_text or not body_text.strip().startswith('{'):
                raise json.JSONDecodeError("ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.", body_text, 0)

            data = json.loads(body_text)
            product_data = data.get('product', {})
            channel_data = product_data.get('channel', {})
            merchant_no = channel_data.get('channelNo')
            origin_product_no = product_data.get('productNo')
            
            if merchant_no and origin_product_no:
                print(f"âœ… APIë¡œ ìƒí’ˆ ì •ë³´ íšë“!")
                print(f"   ğŸ“Š Merchant No: {merchant_no}")
                print(f"   ğŸ“Š Origin Product No: {origin_product_no}")
                return merchant_no, origin_product_no
            else:
                print("âŒ ìƒí’ˆ ì •ë³´(merchant_no, origin_product_no)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
        except Exception as e:
            print(f"âŒ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None, None
    
    def crawl_reviews(self):
        """ë¦¬ë·° í¬ë¡¤ë§"""
        merchant_no, origin_product_no = self.get_product_info()
        if not merchant_no or not origin_product_no:
            return None
        
        all_reviews, page, max_pages = [], 1, 100
        print("ğŸ“ ë¸Œë¼ìš°ì €ë¡œ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
        
        while page <= max_pages:
            try:
                review_url = f"https://smartstore.naver.com/main/products/{origin_product_no}/reviews/writable-reviews?page={page}&sort=REVIEW_RANKING&merchantNo={merchant_no}"
                print(f"ğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
                self.driver.get(review_url)
                self._human_like_delay()
                
                body_text = self.driver.find_element(By.TAG_NAME, "body").text
                if not body_text or not body_text.strip().startswith('{'):
                    print(f"âŒ í˜ì´ì§€ {page}: ì˜¬ë°”ë¥¸ JSON ì‘ë‹µì´ ì•„ë‹™ë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                data = json.loads(body_text)
                reviews = data.get('contents', [])
                if not reviews:
                    print("âœ… ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                    break
                
                for review in reviews:
                    option_contents = review.get('productOptionContents', [])
                    option_text = " / ".join([opt.get('optionContent', '') for opt in option_contents])
                    all_reviews.append({
                        'id': review.get('id'),
                        'rating': review.get('reviewScore'),
                        'writer': review.get('writerMemberId'),
                        'date': review.get('createDate'),
                        'content': review.get('reviewContent', ''),
                        'option': option_text,
                    })
                
                print(f"âœ… í˜ì´ì§€ {page}: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ (ì´ {len(all_reviews)}ê°œ)")
                page += 1
                
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
        
        return pd.DataFrame(all_reviews) if all_reviews else None
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")

if __name__ == '__main__':
    if not SELENIUM_AVAILABLE:
        exit(1)
    
    print("ğŸ¤– === Selenium ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ ì‹œì‘ ===")
    print(f"ğŸ¯ íƒ€ê²Ÿ ìƒí’ˆ: {PRODUCT_ID}")
    
    crawler = None
    try:
        crawler = SeleniumNaverCrawler(PRODUCT_ID, headless=True)
        if not crawler._setup_driver():
            exit(1)
        
        review_df = crawler.crawl_reviews()
        
        if review_df is not None and not review_df.empty:
            print("\nğŸ“Š === ë°ì´í„° ë¶„ì„ ì‹œì‘ ===")
            positive_keywords = ['ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ë¹ ë¥¸', 'í¸í•˜ê³ ', 'ì˜ˆë»ìš”']
            negative_keywords = ['ë¶ˆí¸', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬', 'ë¶ˆë§Œ', 'ëŠë¦°', 'ë¬´ê±°']
            
            review_df['sentiment'] = review_df['content'].apply(lambda x: analyze_sentiment(x, positive_keywords, negative_keywords))
            print("\n--- ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼ ---")
            print(review_df['sentiment'].value_counts())
            
            print("\n--- ğŸ·ï¸ í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---")
            review_df = topic_modeling(review_df, NUM_TOPICS)
            
            review_df.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâŒ === í¬ë¡¤ë§ ì‹¤íŒ¨ ===")
    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if crawler:
            crawler.close()