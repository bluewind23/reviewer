"""
Selenium ê¸°ë°˜ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ëŸ¬
ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ IP ì°¨ë‹¨ì„ ìš°íšŒ
"""

import pandas as pd
import json
import time
import random
from datetime import datetime
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.action_chains import ActionChains
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸  Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install selenium")
    print("Chrome ë“œë¼ì´ë²„ë„ í•„ìš”í•©ë‹ˆë‹¤: https://chromedriver.chromium.org/")

# --- ì„¤ì • ë¶€ë¶„ ---
PRODUCT_ID = "5753732771"
OUTPUT_FILE_NAME = "reviews_selenium.csv"
NUM_TOPICS = 5

class SeleniumNaverCrawler:
    def __init__(self, product_id, headless=True):
        if not SELENIUM_AVAILABLE:
            raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.product_id = product_id
        self.headless = headless
        self.driver = None
        self.wait = None
        
    def _setup_driver(self):
        """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        # ê¸°ë³¸ ì„¤ì •
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        
        # ë´‡ íƒì§€ ìš°íšŒ
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ëœë¤ ì‚¬ìš©ì ì—ì´ì „íŠ¸
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]
        chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        # ì°½ í¬ê¸° ëœë¤í™”
        window_sizes = ["1920,1080", "1366,768", "1440,900", "1536,864"]
        chrome_options.add_argument(f"--window-size={random.choice(window_sizes)}")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.wait = WebDriverWait(self.driver, 30)
            
            print("âœ… Chrome ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("Chrome ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return False
    
    def _human_like_scroll(self):
        """ì¸ê°„ê³¼ ê°™ì€ ìŠ¤í¬ë¡¤ ë™ì‘"""
        # ëœë¤í•œ ìŠ¤í¬ë¡¤ íŒ¨í„´
        scroll_patterns = [
            # ì²œì²œíˆ ì•„ë˜ë¡œ
            lambda: self.driver.execute_script("window.scrollBy(0, 300);"),
            # ë¹ ë¥´ê²Œ ì•„ë˜ë¡œ
            lambda: self.driver.execute_script("window.scrollBy(0, 800);"),
            # ì¡°ê¸ˆ ìœ„ë¡œ
            lambda: self.driver.execute_script("window.scrollBy(0, -200);"),
            # í˜ì´ì§€ ëê¹Œì§€
            lambda: self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"),
        ]
        
        for _ in range(random.randint(2, 5)):
            random.choice(scroll_patterns)()
            time.sleep(random.uniform(0.5, 2.0))
    
    def _human_like_delay(self):
        """ì¸ê°„ê³¼ ê°™ì€ ì§€ì—°"""
        delay = random.uniform(2, 5)
        print(f"â³ {delay:.2f}ì´ˆ ëŒ€ê¸°...")
        time.sleep(delay)
    
    def _simulate_human_activity(self):
        """ì¸ê°„ í™œë™ ì‹œë®¬ë ˆì´ì…˜"""
        activities = [
            self._random_mouse_move,
            self._random_scroll,
            self._pause_and_read,
        ]
        
        if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ í™œë™
            activity = random.choice(activities)
            try:
                activity()
            except:
                pass
    
    def _random_mouse_move(self):
        """ëœë¤ ë§ˆìš°ìŠ¤ ì´ë™"""
        try:
            actions = ActionChains(self.driver)
            actions.move_by_offset(random.randint(-100, 100), random.randint(-100, 100))
            actions.perform()
            time.sleep(random.uniform(0.1, 0.5))
        except:
            pass
    
    def _random_scroll(self):
        """ëœë¤ ìŠ¤í¬ë¡¤"""
        scroll_amount = random.randint(100, 500)
        self.driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(random.uniform(0.5, 1.5))
    
    def _pause_and_read(self):
        """ì½ê¸° ì‹œë®¬ë ˆì´ì…˜"""
        time.sleep(random.uniform(3, 8))
    
    def get_product_info(self):
        """ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        print("ğŸ” ë¸Œë¼ìš°ì €ë¡œ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # ìƒí’ˆ í˜ì´ì§€ ì ‘ì†
            product_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}"
            self.driver.get(product_url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            self._human_like_delay()
            
            # ì¸ê°„ í™œë™ ì‹œë®¬ë ˆì´ì…˜
            self._simulate_human_activity()
            
            # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì‹œë„
            merchant_no = None
            origin_product_no = None
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ ì‹œë„
            try:
                # ë°©ë²• 1: í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ JSON ë°ì´í„° ì°¾ê¸°
                page_source = self.driver.page_source
                
                # JSON ë°ì´í„° íŒ¨í„´ ê²€ìƒ‰
                import re
                json_pattern = r'"productNo":\s*"?(\d+)"?'
                match = re.search(json_pattern, page_source)
                if match:
                    origin_product_no = match.group(1)
                
                merchant_pattern = r'"channelNo":\s*"?(\d+)"?'
                match = re.search(merchant_pattern, page_source)
                if match:
                    merchant_no = match.group(1)
                
                if merchant_no and origin_product_no:
                    print(f"âœ… ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì„±ê³µ!")
                    print(f"   ğŸ“Š Merchant No: {merchant_no}")
                    print(f"   ğŸ“Š Origin Product No: {origin_product_no}")
                    return merchant_no, origin_product_no
                
            except Exception as e:
                print(f"âš ï¸  ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ë°©ë²• 2: API í˜¸ì¶œë¡œ ì •ë³´ ì–»ê¸°
            try:
                api_url = f"https://smartstore.naver.com/i/v1/products/{self.product_id}/summary"
                self.driver.get(api_url)
                time.sleep(3)
                
                # JSON ì‘ë‹µ íŒŒì‹±
                body_text = self.driver.find_element(By.TAG_NAME, "body").text
                
                if body_text and body_text.strip().startswith('{'):
                    data = json.loads(body_text)
                    merchant_no = data['product']['channel']['channelNo']
                    origin_product_no = data['product']['productNo']
                    
                    print(f"âœ… APIë¡œ ìƒí’ˆ ì •ë³´ íšë“!")
                    print(f"   ğŸ“Š Merchant No: {merchant_no}")
                    print(f"   ğŸ“Š Origin Product No: {origin_product_no}")
                    return merchant_no, origin_product_no
                    
            except Exception as e:
                print(f"âš ï¸  API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            print("âŒ ìƒí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None, None
            
        except Exception as e:
            print(f"âŒ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None, None
    
    def crawl_reviews(self):
        """ë¦¬ë·° í¬ë¡¤ë§"""
        merchant_no, origin_product_no = self.get_product_info()
        
        if not merchant_no or not origin_product_no:
            return None
        
        all_reviews = []
        page = 1
        max_pages = 100  # ìµœëŒ€ í˜ì´ì§€ ì œí•œ
        
        print("ğŸ“ ë¸Œë¼ìš°ì €ë¡œ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
        
        while page <= max_pages:
            try:
                # ë¦¬ë·° API URL ì ‘ì†
                review_url = (
                    f"https://smartstore.naver.com/main/products/{origin_product_no}/reviews/writable-reviews"
                    f"?page={page}&sort=REVIEW_RANKING&merchantNo={merchant_no}"
                )
                
                print(f"ğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
                self.driver.get(review_url)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                self._human_like_delay()
                
                # JSON ì‘ë‹µ íŒŒì‹±
                try:
                    body_text = self.driver.find_element(By.TAG_NAME, "body").text
                    
                    if not body_text or not body_text.strip().startswith('{'):
                        print(f"âŒ í˜ì´ì§€ {page}: ì˜¬ë°”ë¥¸ JSON ì‘ë‹µì´ ì•„ë‹™ë‹ˆë‹¤")
                        break
                    
                    data = json.loads(body_text)
                    reviews = data.get('contents', [])
                    
                    if not reviews:
                        print("âœ… ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")
                        break
                    
                    for review in reviews:
                        option_text = " / ".join([
                            opt['optionContent'] 
                            for opt in review.get('productOptionContents', []) 
                            if 'optionContent' in opt
                        ])
                        
                        all_reviews.append({
                            'id': review.get('id'),
                            'rating': review.get('reviewScore'),
                            'writer': review.get('writerMemberId'),
                            'date': review.get('createDate'),
                            'content': review.get('reviewContent', ''),
                            'option': option_text,
                        })
                    
                    print(f"âœ… í˜ì´ì§€ {page}: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ (ì´ {len(all_reviews)}ê°œ)")
                    page += 1
                    
                    # ì¸ê°„ í™œë™ ì‹œë®¬ë ˆì´ì…˜
                    self._simulate_human_activity()
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                    print(f"ì‘ë‹µ ë‚´ìš©: {body_text[:200]}...")
                    break
                except Exception as e:
                    print(f"âŒ ë¦¬ë·° íŒŒì‹± ì˜¤ë¥˜: {e}")
                    break
                    
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        if all_reviews:
            print(f"ğŸ‰ ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            return pd.DataFrame(all_reviews)
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def analyze_sentiment(text, positive_keywords, negative_keywords):
    """ê°ì„± ë¶„ì„"""
    score = 0
    for keyword in positive_keywords:
        if keyword in text:
            score += 1
    for keyword in negative_keywords:
        if keyword in text:
            score -= 1
    
    if score > 0:
        return 'ê¸ì •'
    elif score < 0:
        return 'ë¶€ì •'
    else:
        return 'ì¤‘ë¦½'


def topic_modeling(df, num_topics):
    """í† í”½ ëª¨ë¸ë§"""
    okt = Okt()
    
    def tokenize(text):
        if isinstance(text, str):
            return [token for token in okt.nouns(text) if len(token) > 1]
        return []

    df['tokens'] = df['content'].apply(tokenize)
    texts = [" ".join(tokens) for tokens in df['tokens']]

    if not any(texts):
        print("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. í† í”½ ëª¨ë¸ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        df['topic'] = 'ë¶„ì„ ë¶ˆê°€'
        return df

    vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2)
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(tfidf_matrix)
    
    topic_results = lda.transform(tfidf_matrix)
    df['topic'] = topic_results.argmax(axis=1)

    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(lda.components_):
        top_keywords = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]
        print(f"í† í”½ #{topic_idx}: {', '.join(top_keywords)}")
        
    return df


if __name__ == '__main__':
    if not SELENIUM_AVAILABLE:
        print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ë°©ë²•:")
        print("1. pip install selenium")
        print("2. Chrome ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ: https://chromedriver.chromium.org/")
        print("3. Chrome ë“œë¼ì´ë²„ë¥¼ PATHì— ì¶”ê°€í•˜ê±°ë‚˜ í˜„ì¬ í´ë”ì— ì €ì¥")
        exit(1)
    
    print("ğŸ¤– === Selenium ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ ì‹œì‘ ===")
    print(f"ğŸ¯ íƒ€ê²Ÿ ìƒí’ˆ: {PRODUCT_ID}")
    print("ğŸ”§ Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
    
    crawler = None
    try:
        # í¬ë¡¤ëŸ¬ ìƒì„± (headless=Falseë¡œ í•˜ë©´ ë¸Œë¼ìš°ì €ë¥¼ ë³¼ ìˆ˜ ìˆìŒ)
        crawler = SeleniumNaverCrawler(PRODUCT_ID, headless=True)
        
        if not crawler._setup_driver():
            print("âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨")
            exit(1)
        
        # ë¦¬ë·° í¬ë¡¤ë§
        review_df = crawler.crawl_reviews()
        
        if review_df is not None and not review_df.empty:
            print("\nğŸ“Š === ë°ì´í„° ë¶„ì„ ì‹œì‘ ===")
            
            # ê°ì„± ë¶„ì„
            positive_keywords = ['ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ë¹ ë¥¸', 'í¸í•˜ê³ ', 'ì˜ˆë»ìš”', 'ê°€ë³ê³ ', 'íŠ¼íŠ¼', 'ì˜', 'ë§˜ì—']
            negative_keywords = ['ë¶ˆí¸', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì•„ì‰¬', 'ë¶ˆë§Œ', 'ëŠë¦°', 'ë¬´ê±°', 'ì•½í•œ', 'ë¬¸ì œ', 'ê³ ì¥']
            
            review_df['sentiment'] = review_df['content'].apply(
                lambda x: analyze_sentiment(x, positive_keywords, negative_keywords)
            )
            
            print("\n--- ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼ ---")
            print(review_df['sentiment'].value_counts())
            
            # í† í”½ ëª¨ë¸ë§
            print("\n--- ğŸ·ï¸  í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---")
            review_df = topic_modeling(review_df, NUM_TOPICS)
            
            # ê²°ê³¼ ì €ì¥
            review_df.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{OUTPUT_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            # í†µê³„ ì •ë³´
            print(f"\n--- ğŸ“Š ìˆ˜ì§‘ í†µê³„ ---")
            print(f"ğŸ“ ì´ ë¦¬ë·° ìˆ˜: {len(review_df)}")
            print(f"â­ í‰ê·  í‰ì : {review_df['rating'].mean():.2f}")
            print(f"ğŸ˜Š ê¸ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ê¸ì •'])}")
            print(f"ğŸ˜ ë¶€ì • ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ë¶€ì •'])}")
            print(f"ğŸ˜ ì¤‘ë¦½ ë¦¬ë·°: {len(review_df[review_df['sentiment'] == 'ì¤‘ë¦½'])}")
            
        else:
            print("\nâŒ === í¬ë¡¤ë§ ì‹¤íŒ¨ ===")
            print("ğŸ”§ í•´ê²° ë°©ì•ˆ:")
            print("1. ìƒí’ˆ ID í™•ì¸")
            print("2. Chrome ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸")
            print("3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
    
    except KeyboardInterrupt:
        print("\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        if crawler:
            crawler.close()